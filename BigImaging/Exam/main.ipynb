{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import albumentations as A\n",
    "from tqdm.auto import tqdm\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 2)\n",
      "['data/original_images/000.jpg' 'data/label_images_semantic/000.png']\n"
     ]
    }
   ],
   "source": [
    "# a list to collect paths of images\n",
    "images_path = []\n",
    "labels_path = []\n",
    "masks_path = []\n",
    "\n",
    "# Get the paths of the images and sort them\n",
    "images_path = sorted(glob.glob('data/original_images/*.jpg'))\n",
    "labels_path = sorted(glob.glob('data/label_images_semantic/*.png'))\n",
    "rgb_masks_path = sorted(glob.glob('data/RGB_color_image_masks/*.png'))\n",
    "\n",
    "paths = np.column_stack((images_path, labels_path))\n",
    "print(paths.shape)\n",
    "print(paths[0])\n",
    "\n",
    "# Apply 80-10-10 split\n",
    "train_split, valtest_split = train_test_split(paths, test_size=0.2, random_state=69420)\n",
    "val_split, test_split = train_test_split(valtest_split, test_size=0.5, random_state=69420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def create_and_save_tiles(img_path, mask_path, tiles_dim=512, final_dim=256, output_dir=\"data\"):\n",
    "    # Extract the original index from the image path\n",
    "    original_index = os.path.basename(img_path)[:3]\n",
    "\n",
    "    # Load image and mask\n",
    "    img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # Convert to tensor\n",
    "    transform = transforms.ToTensor()\n",
    "    img = transform(img)\n",
    "    mask = transform(mask)\n",
    "\n",
    "    # Check if resizing is necessary\n",
    "    if img.shape[1] % tiles_dim != 0 or img.shape[2] % tiles_dim != 0:\n",
    "        # Round down to the nearest multiple of tiles_dim\n",
    "        new_height = img.shape[1] // tiles_dim * tiles_dim\n",
    "        new_width = img.shape[2] // tiles_dim * tiles_dim\n",
    "        new_shp = (new_height, new_width)\n",
    "\n",
    "        # Resize the image and mask\n",
    "        img = F.interpolate(img.unsqueeze(0), size=new_shp, mode='bilinear', align_corners=False).squeeze(0)\n",
    "        mask = F.interpolate(mask.unsqueeze(0), size=new_shp, mode='nearest').squeeze(0)\n",
    "  \n",
    "    img_tiles = img.unfold(1, tiles_dim, tiles_dim).unfold(2, tiles_dim, tiles_dim)\n",
    "    img_tiles = img_tiles.contiguous().view(3, -1, tiles_dim, tiles_dim).permute(1, 0, 2, 3)\n",
    "    \n",
    "    mask_tiles = mask.unfold(1, tiles_dim, tiles_dim).unfold(2, tiles_dim, tiles_dim)\n",
    "    mask_tiles = mask_tiles.contiguous().view(-1, tiles_dim, tiles_dim)\n",
    "\n",
    "    # Resize tiles to 256x256\n",
    "    resize_dim = final_dim\n",
    "    img_tiles = F.interpolate(img_tiles, size=(resize_dim, resize_dim), mode='bilinear', align_corners=False)\n",
    "    mask_tiles = F.interpolate(mask_tiles.unsqueeze(1), size=(resize_dim, resize_dim), mode='nearest').squeeze(1)\n",
    "\n",
    "    # Create output directories if they don't exist\n",
    "    img_output_dir = os.path.join(output_dir, f\"{tiles_dim}x{tiles_dim}\", \"images\")\n",
    "    mask_output_dir = os.path.join(output_dir, f\"{tiles_dim}x{tiles_dim}\", \"masks\")\n",
    "    os.makedirs(img_output_dir, exist_ok=True)\n",
    "    os.makedirs(mask_output_dir, exist_ok=True)\n",
    "\n",
    "    # Save tiles\n",
    "    for i, (img_tile, mask_tile) in enumerate(zip(img_tiles, mask_tiles)):\n",
    "        img_tile_path = os.path.join(img_output_dir, f\"{original_index}_{i}.png\")\n",
    "        mask_tile_path = os.path.join(mask_output_dir, f\"{original_index}_{i}.png\")\n",
    "        save_image(img_tile, img_tile_path)\n",
    "        save_image(mask_tile, mask_tile_path)\n",
    "\n",
    "    return img_tiles, mask_tiles\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "already_created = True\n",
    "\n",
    "if not already_created:\n",
    "    resize_dim = 256\n",
    "    for path in tqdm(paths):\n",
    "        img_path, mask_path = path\n",
    "        create_and_save_tiles(img_path, mask_path, tiles_dim=2000, final_dim=256, output_dir=f'data/tiles_{resize_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for img in tqdm(paths[:, 0]):\n",
    "#     shape = cv2.imread(img).shape\n",
    "#     print(f'shape for img {img} is {shape}')\n",
    "#     if shape != (4000, 6000, 3):\n",
    "#         print(f'wrong shape for img {img}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>r</th>\n",
       "      <th>g</th>\n",
       "      <th>b</th>\n",
       "      <th>RGB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unlabeled</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(0, 0, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>paved-area</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>(128, 64, 128)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dirt</td>\n",
       "      <td>130</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>(130, 76, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>grass</td>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "      <td>(0, 102, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gravel</td>\n",
       "      <td>112</td>\n",
       "      <td>103</td>\n",
       "      <td>87</td>\n",
       "      <td>(112, 103, 87)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>water</td>\n",
       "      <td>28</td>\n",
       "      <td>42</td>\n",
       "      <td>168</td>\n",
       "      <td>(28, 42, 168)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rocks</td>\n",
       "      <td>48</td>\n",
       "      <td>41</td>\n",
       "      <td>30</td>\n",
       "      <td>(48, 41, 30)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pool</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>89</td>\n",
       "      <td>(0, 50, 89)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>vegetation</td>\n",
       "      <td>107</td>\n",
       "      <td>142</td>\n",
       "      <td>35</td>\n",
       "      <td>(107, 142, 35)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>roof</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>(70, 70, 70)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>wall</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>156</td>\n",
       "      <td>(102, 102, 156)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>window</td>\n",
       "      <td>254</td>\n",
       "      <td>228</td>\n",
       "      <td>12</td>\n",
       "      <td>(254, 228, 12)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>door</td>\n",
       "      <td>254</td>\n",
       "      <td>148</td>\n",
       "      <td>12</td>\n",
       "      <td>(254, 148, 12)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>fence</td>\n",
       "      <td>190</td>\n",
       "      <td>153</td>\n",
       "      <td>153</td>\n",
       "      <td>(190, 153, 153)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>fence-pole</td>\n",
       "      <td>153</td>\n",
       "      <td>153</td>\n",
       "      <td>153</td>\n",
       "      <td>(153, 153, 153)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>person</td>\n",
       "      <td>255</td>\n",
       "      <td>22</td>\n",
       "      <td>96</td>\n",
       "      <td>(255, 22, 96)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>dog</td>\n",
       "      <td>102</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>(102, 51, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>car</td>\n",
       "      <td>9</td>\n",
       "      <td>143</td>\n",
       "      <td>150</td>\n",
       "      <td>(9, 143, 150)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bicycle</td>\n",
       "      <td>119</td>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "      <td>(119, 11, 32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>tree</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>(51, 51, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>bald-tree</td>\n",
       "      <td>190</td>\n",
       "      <td>250</td>\n",
       "      <td>190</td>\n",
       "      <td>(190, 250, 190)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ar-marker</td>\n",
       "      <td>112</td>\n",
       "      <td>150</td>\n",
       "      <td>146</td>\n",
       "      <td>(112, 150, 146)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>obstacle</td>\n",
       "      <td>2</td>\n",
       "      <td>135</td>\n",
       "      <td>115</td>\n",
       "      <td>(2, 135, 115)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>conflicting</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(255, 0, 0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          class    r    g    b              RGB\n",
       "0     unlabeled    0    0    0        (0, 0, 0)\n",
       "1    paved-area  128   64  128   (128, 64, 128)\n",
       "2          dirt  130   76    0     (130, 76, 0)\n",
       "3         grass    0  102    0      (0, 102, 0)\n",
       "4        gravel  112  103   87   (112, 103, 87)\n",
       "5         water   28   42  168    (28, 42, 168)\n",
       "6         rocks   48   41   30     (48, 41, 30)\n",
       "7          pool    0   50   89      (0, 50, 89)\n",
       "8    vegetation  107  142   35   (107, 142, 35)\n",
       "9          roof   70   70   70     (70, 70, 70)\n",
       "10         wall  102  102  156  (102, 102, 156)\n",
       "11       window  254  228   12   (254, 228, 12)\n",
       "12         door  254  148   12   (254, 148, 12)\n",
       "13        fence  190  153  153  (190, 153, 153)\n",
       "14   fence-pole  153  153  153  (153, 153, 153)\n",
       "15       person  255   22   96    (255, 22, 96)\n",
       "16          dog  102   51    0     (102, 51, 0)\n",
       "17          car    9  143  150    (9, 143, 150)\n",
       "18      bicycle  119   11   32    (119, 11, 32)\n",
       "19         tree   51   51    0      (51, 51, 0)\n",
       "20    bald-tree  190  250  190  (190, 250, 190)\n",
       "21    ar-marker  112  150  146  (112, 150, 146)\n",
       "22     obstacle    2  135  115    (2, 135, 115)\n",
       "23  conflicting  255    0    0      (255, 0, 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read number of classes\n",
    "labels_colors = pd.read_csv('data/class_dict_seg.csv')\n",
    "columns = ['class', 'r', 'g', 'b']\n",
    "labels_colors.columns = columns\n",
    "# Extract RGB values\n",
    "labels_colors['RGB'] = labels_colors[['r', 'g', 'b']].apply(tuple, axis=1)\n",
    "colors = labels_colors['RGB'].values\n",
    "labels_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if there are any conflicting labels in the masks\n",
    "# for img in tqdm(paths[:, 1]):\n",
    "#     mask = cv2.imread(img, cv2.IMREAD_GRAYSCALE)\n",
    "#     unlabelled = np.sum(mask == 23)\n",
    "#     if unlabelled > 0:\n",
    "#         print(f'Unlabelled pixels in img {img}: {unlabelled} ({unlabelled / mask.size * 100:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 23\n"
     ]
    }
   ],
   "source": [
    "# No conflicting labels found\n",
    "# Therefore there are 23 classes in the dataset\n",
    "nr_classes = len(labels_colors) - 1\n",
    "print(f'Number of classes: {nr_classes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TilesDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None, tiles=True, tiles_dim=512):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "        self.tiles = tiles\n",
    "        self.tiles_dim = tiles_dim\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, mask_path = self.image_paths[idx]\n",
    "        img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            augmented = self.transform(image=img, mask=mask)\n",
    "            img, mask = augmented['image'], augmented['mask']\n",
    "\n",
    "        if self.tiles:\n",
    "            img, mask = self.create_tiles(img, mask, self.tiles_dim)\n",
    "\n",
    "        return img, mask\n",
    "    \n",
    "    def create_tiles(self, img, mask, tiles_dim):\n",
    "        # Check if resizing is necessary\n",
    "        if img.shape[1] % tiles_dim != 0 or img.shape[2] % tiles_dim != 0:\n",
    "            # Round down to the nearest multiple of tiles_dim\n",
    "            new_height = img.shape[1] // tiles_dim * tiles_dim\n",
    "            new_width = img.shape[2] // tiles_dim * tiles_dim\n",
    "            new_shp = (new_height, new_width)\n",
    "\n",
    "            # Resize the image and mask\n",
    "            img = F.interpolate(img.unsqueeze(0), size=new_shp, mode='bilinear', align_corners=False).squeeze(0)\n",
    "            mask = F.interpolate(mask.unsqueeze(0).unsqueeze(0), size=new_shp, mode='nearest').squeeze(0).squeeze(0)\n",
    "\n",
    "            # Print the shape of the image and mask\n",
    "            print(f\"Image shape: {img.shape}\")\n",
    "            print(f\"Mask shape: {mask.shape}\")\n",
    "        \n",
    "        # Create img tiles and mask tiles\n",
    "        img_tiles = img.unfold(1, tiles_dim, tiles_dim).unfold(2, tiles_dim, tiles_dim)\n",
    "        img_tiles = img_tiles.contiguous().view(3, -1, tiles_dim, tiles_dim).permute(1, 0, 2, 3)\n",
    "        \n",
    "        mask_tiles = mask.unfold(0, tiles_dim, tiles_dim).unfold(1, tiles_dim, tiles_dim)\n",
    "        mask_tiles = mask_tiles.contiguous().view(-1, tiles_dim, tiles_dim)\n",
    "\n",
    "        # Resize tiles if necessary\n",
    "        if tiles_dim > 256:\n",
    "            resize_dim = 256\n",
    "            img_tiles = F.interpolate(img_tiles, size=(resize_dim, resize_dim), mode='bilinear', align_corners=False)\n",
    "            mask_tiles = F.interpolate(mask_tiles.unsqueeze(1), size=(resize_dim, resize_dim), mode='nearest').squeeze(1)\n",
    "\n",
    "        return img_tiles, mask_tiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get image and mask paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30800, 2)\n",
      "['data/tiles_256/512x512/images/000_0.png'\n",
      " 'data/tiles_256/512x512/masks/000_0.png']\n"
     ]
    }
   ],
   "source": [
    "final_dim = 256\n",
    "tiles_dim = 512\n",
    "tiles_path = f'data/tiles_{final_dim}/{tiles_dim}x{tiles_dim}'\n",
    "\n",
    "# Get image paths\n",
    "for folder in os.listdir(tiles_path):\n",
    "    if folder == 'images':\n",
    "        img_paths = sorted(glob.glob(f'{tiles_path}/{folder}/*.png'))\n",
    "    elif folder == 'masks':\n",
    "        mask_paths = sorted(glob.glob(f'{tiles_path}/{folder}/*.png'))\n",
    "\n",
    "# Combine image and mask paths\n",
    "image_paths = np.array(list(zip(img_paths, mask_paths)))\n",
    "print(image_paths.shape)\n",
    "print(image_paths[0])\n",
    "\n",
    "# Apply 80-10-10 split\n",
    "train_split, valtest_split = train_test_split(image_paths, test_size=0.2, random_state=69420)\n",
    "val_split, test_split = train_test_split(valtest_split, test_size=0.5, random_state=69420)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 24640, Val dataset length: 3080, Test dataset length: 3080\n"
     ]
    }
   ],
   "source": [
    "# Define Albumentations transformations\n",
    "train_transform = A.Compose([\n",
    "    # A.Resize(new_height, new_width, p=1.0),  # Resize the image to the desired shape\n",
    "    A.HorizontalFlip(p=0.5),  # Apply horizontal flip with 50% probability\n",
    "    A.VerticalFlip(p=0.5),  # Apply vertical flip with 50% probability\n",
    "    A.RandomBrightnessContrast(p=0.2),  # Randomly change brightness and contrast\n",
    "    A.OneOf([\n",
    "        A.GaussianBlur(p=1.0),  # Apply Gaussian blur\n",
    "        A.MotionBlur(p=1.0),  # Apply motion blur\n",
    "    ], p=0.2),  # Apply one of the blur operations with 20% probability\n",
    "    A.HueSaturationValue(p=0.2),  # Randomly change hue, saturation, and value\n",
    "    A.RandomGamma(p=0.2),  # Randomly change gamma\n",
    "    A.CLAHE(p=0.2),  # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # Normalize the image\n",
    "    ToTensorV2(),  # Convert image and mask to PyTorch tensors\n",
    "])\n",
    "\n",
    "valtest_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),  # Apply horizontal flip with 50% probability\n",
    "    A.VerticalFlip(p=0.5),  # Apply vertical flip with 50% probability\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # Normalize the image\n",
    "    ToTensorV2(),  # Convert image and mask to PyTorch tensors\n",
    "])\n",
    "\n",
    "# Initialize your custom dataset\n",
    "train_ds = TilesDataset(train_split, transform=train_transform, tiles_dim=tiles_dim, tiles=False)\n",
    "val_ds = TilesDataset(val_split, transform=valtest_transform, tiles_dim=tiles_dim, tiles=False)\n",
    "test_ds = TilesDataset(test_split, transform=valtest_transform, tiles_dim=tiles_dim, tiles=False)\n",
    "\n",
    "print(f'Train dataset length: {len(train_ds)}, Val dataset length: {len(val_ds)}, Test dataset length: {len(test_ds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader\n",
    "num_workers = 12\n",
    "batch_size_train = 90\n",
    "batch_size_valtest = 75\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size_train, shuffle=True, pin_memory=True, num_workers=num_workers, persistent_workers=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size_valtest, shuffle=False, pin_memory=True, num_workers=num_workers, persistent_workers=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size_valtest, shuffle=False, pin_memory=True, num_workers=num_workers, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_torch(y_true, y_pred, num_classes):\n",
    "    # Flatten the arrays for metric computation\n",
    "    y_true_flat = y_true.view(-1)\n",
    "    y_pred_flat = y_pred.view(-1)\n",
    "\n",
    "    # Compute overall accuracy\n",
    "    acc = (y_true_flat == y_pred_flat).float().mean().item()\n",
    "\n",
    "    # Helper function to compute IoU for a single class\n",
    "    def compute_iou(cls):\n",
    "        intersection = ((y_true_flat == cls) & (y_pred_flat == cls)).float().sum().item()\n",
    "        union = ((y_true_flat == cls) | (y_pred_flat == cls)).float().sum().item()\n",
    "        return intersection / union if union != 0 else 0\n",
    "\n",
    "    # Helper function to compute Dice score for a single class\n",
    "    def compute_dice(cls):\n",
    "        intersection = 2 * ((y_true_flat == cls) & (y_pred_flat == cls)).float().sum().item()\n",
    "        total = (y_true_flat == cls).float().sum().item() + (y_pred_flat == cls).float().sum().item()\n",
    "        return intersection / total if total != 0 else 0\n",
    "\n",
    "    # Compute IoU\n",
    "    iou_list = [compute_iou(cls) for cls in range(num_classes)]\n",
    "    mean_iou = np.mean(iou_list)\n",
    "\n",
    "    # Compute Dice\n",
    "    dice_list = [compute_dice(cls) for cls in range(num_classes)]\n",
    "    mean_dice = np.mean(dice_list)\n",
    "\n",
    "    # Return the metrics\n",
    "    return {\n",
    "        'mean_iou': mean_iou,\n",
    "        'per_class_iou': iou_list,\n",
    "        'accuracy': acc,\n",
    "        'mean_dice': mean_dice,\n",
    "        'per_class_dice': dice_list\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'arch': 'unet',\n",
    "    'encoder_name': 'resnet34',\n",
    "    'encoder_weights': 'imagenet',\n",
    "    'in_channels': 3,\n",
    "    'classes': nr_classes\n",
    "}\n",
    "\n",
    "model = smp.create_model(**config)\n",
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 17:25:14.038262: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-30 17:25:14.054619: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-30 17:25:14.059659: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-30 17:25:14.071642: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-30 17:25:14.862736: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard logs directory: /home/andrea/Documents/unimib/BigImaging/Exam/logs/unet/resnet34/tiles_256/512x512/2024-07-30_17-25-15\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "\n",
    "# Create a TensorBoard callback\n",
    "logs_dir = f'logs/{config[\"arch\"]}/{config[\"encoder_name\"]}/tiles_{final_dim}/{tiles_dim}x{tiles_dim}/{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}'\n",
    "os.makedirs(logs_dir, exist_ok=True)  # Ensure the logs directory exists\n",
    "# Get full path to the logs directory\n",
    "logs_dir = os.path.abspath(logs_dir)\n",
    "print(f\"TensorBoard logs directory: {logs_dir}\")\n",
    "\n",
    "writer = SummaryWriter(log_dir=logs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.amp.grad_scaler import GradScaler\n",
    "from torch.amp.autocast_mode import autocast\n",
    "\n",
    "def reshape_imgs_masks(imgs, masks):\n",
    "    imgs, masks = imgs.to(device), masks.to(device)\n",
    "\n",
    "    # Reshape images: [batch_size, num_tiles, channels, height, width] -> [batch_size * num_tiles, channels, height, width]\n",
    "    imgs = imgs.view(-1, imgs.shape[2], imgs.shape[3], imgs.shape[4])\n",
    "    # Reshape masks: [batch_size, num_tiles, height, width] -> [batch_size * num_tiles, height, width]\n",
    "    masks = masks.view(-1, masks.shape[2], masks.shape[3])\n",
    "\n",
    "    # Convert masks to Long() type\n",
    "    masks = masks.to(torch.long)\n",
    "\n",
    "    return imgs,masks\n",
    "\n",
    "def train(train_loss, imgs, masks, scaler, optimizer, criterion, iteration, accumulation_steps=1, use_amp=True, tiles=False):\n",
    "    \n",
    "    # Only reshape images and masks if tiles are being computed by the Dataset class\n",
    "    # Else the source is the already tiled images and masks\n",
    "    if tiles:\n",
    "        imgs, masks = reshape_imgs_masks(imgs, masks)\n",
    "    else:\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        masks = masks.to(torch.long)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if use_amp:\n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, masks) / accumulation_steps\n",
    "        scaler.scale(loss).backward()\n",
    "    else:\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, masks) / accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "    if (iteration + 1) % accumulation_steps == 0:\n",
    "        if use_amp:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    train_loss += loss.detach() * accumulation_steps  # Adjust for scaled loss\n",
    "\n",
    "    if (iteration + 1) % 64 == 0:\n",
    "        print(f'Train loss at iteration {iteration + 1}: {train_loss.item() / iteration :.3f}')\n",
    "\n",
    "    return train_loss\n",
    "\n",
    "def validate(val_loss, imgs, masks, criterion, use_amp=True, tiles=False):\n",
    "    with torch.no_grad():\n",
    "        if tiles:\n",
    "            imgs, masks = reshape_imgs_masks(imgs, masks)\n",
    "        else:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            masks = masks.to(torch.long)\n",
    "\n",
    "        if use_amp:\n",
    "            with autocast(device_type='cuda'):\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.detach()\n",
    "        else:\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, masks)\n",
    "            val_loss += loss.detach()\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "    return val_loss, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(all_y_true.device)\n",
    "print(all_y_pred.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b619b1b0ef44d07b7ba3efe74132dd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1/100:   0%|          | 0/274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss at iteration 64: 1.696\n",
      "Train loss at iteration 128: 1.380\n",
      "Train loss at iteration 192: 1.254\n",
      "Train loss at iteration 256: 1.173\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3df2dba4a3e403c879c975224ccb202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch 1/100:   0%|          | 0/42 [00:01<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m all_y_true \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(all_y_true, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     48\u001b[0m all_y_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(all_y_pred, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_metrics_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_y_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_y_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(val_loader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Mean IoU: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_iou\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Dice Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_dice\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     54\u001b[0m   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mper-class IoU: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miou\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mi,\u001b[38;5;250m \u001b[39miou\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28menumerate\u001b[39m(metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mper_class_iou\u001b[39m\u001b[38;5;124m\"\u001b[39m])]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m, in \u001b[0;36mcompute_metrics_torch\u001b[0;34m(y_true, y_pred, num_classes)\u001b[0m\n\u001b[1;32m      4\u001b[0m y_pred_flat \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Compute overall accuracy\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m acc \u001b[38;5;241m=\u001b[39m (\u001b[43my_true_flat\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_pred_flat\u001b[49m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Helper function to compute IoU for a single class\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_iou\u001b[39m(\u001b[38;5;28mcls\u001b[39m):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "# Set up mixed precision training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Accumulation steps (adjust based on GPU memory)\n",
    "accumulation_steps = 1\n",
    "\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 100\n",
    "num_classes = nr_classes\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    if epoch == 0:\n",
    "        with torch.profiler.profile(\n",
    "            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "            schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),\n",
    "            on_trace_ready=torch.profiler.tensorboard_trace_handler(logs_dir),\n",
    "            record_shapes=True, profile_memory=True, with_stack=True\n",
    "        ) as prof:\n",
    "            for i, (imgs, masks) in enumerate(tqdm(train_loader, desc=f'Training Epoch {epoch+1}/{num_epochs}')):\n",
    "                train_loss = train(train_loss, imgs, masks, scaler, optimizer, criterion, i, accumulation_steps, use_amp=True)\n",
    "                # Profile each 2 batches\n",
    "                if i % 2 == 0:\n",
    "                    prof.step()\n",
    "    else:\n",
    "        for i, (imgs, masks) in enumerate(tqdm(train_loader, desc=f'Training Epoch {epoch+1}/{num_epochs}')):\n",
    "            train_loss = train(train_loss, imgs, masks, scaler, optimizer, criterion, i, accumulation_steps, use_amp=True)\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks in tqdm(val_loader, desc=f'Validation Epoch {epoch+1}/{num_epochs}'):\n",
    "            val_loss, preds = validate(val_loss, imgs, masks, criterion, use_amp=True)\n",
    "            all_y_true.append(masks.to(device))\n",
    "            all_y_pred.append(preds)\n",
    "\n",
    "    all_y_true = torch.cat(all_y_true, dim=0)\n",
    "    all_y_pred = torch.cat(all_y_pred, dim=0)\n",
    "\n",
    "    metrics = compute_metrics_torch(all_y_true, all_y_pred, num_classes)\n",
    "\n",
    "    print(f'Validation Loss: {val_loss.item()/len(val_loader):.3f}, Mean IoU: {metrics[\"mean_iou\"]:.3f}, '\n",
    "      f'Accuracy: {metrics[\"accuracy\"]:.3f}, Dice Score: {metrics[\"mean_dice\"]:.3f}, '\n",
    "      f'per-class IoU: {[f\"Class {i}: {iou:.3f}\" for i, iou in enumerate(metrics[\"per_class_iou\"])]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def test(testloader, model, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, masks in tqdm(testloader, desc=f'Test'):\n",
    "            imgs, masks = reshape_imgs_masks(imgs, masks)\n",
    "\n",
    "            with autocast(device_type='cuda'):\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, masks)\n",
    "                test_loss += loss.detach()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_y_true.append(masks)\n",
    "            all_y_pred.append(preds)\n",
    "\n",
    "    all_y_true_flattened = torch.cat(all_y_true, dim=0)\n",
    "    all_y_pred_flattened = torch.cat(all_y_pred, dim=0)\n",
    "\n",
    "    print(f'All y true shape: {all_y_true_flattened.shape}, All y pred shape: {all_y_pred_flattened.shape}')\n",
    "\n",
    "    metrics = compute_metrics_torch(all_y_true_flattened, all_y_pred_flattened, num_classes)\n",
    "\n",
    "    print(f'Test Loss: {test_loss.item()/len(testloader):.3f}, Mean IoU: {metrics[\"mean_iou\"]:.3f}, '\n",
    "        f'Accuracy: {metrics[\"accuracy\"]:.3f}, Dice Score: {metrics[\"mean_dice\"]:.3f}, '\n",
    "        f'per-class IoU: {[f\"Class {i}: {iou:.3f}\" for i, iou in enumerate(metrics[\"per_class_iou\"])]}')\n",
    "    \n",
    "    return all_y_true_flattened, all_y_pred_flattened\n",
    "\n",
    "def convert_to_rgb(masks, colors):\n",
    "    \"\"\"\n",
    "    Convert a 4D tensor of masks to a 5D tensor of RGB masks.\n",
    "\n",
    "    Args:\n",
    "        masks (Tensor): A 4D tensor of masks with shape [batch_size, num_tiles, height, width].\n",
    "        colors (Tensor): A tensor of RGB color values for each class.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: A 5D tensor of RGB masks with shape [batch_size, num_tiles, 3, height, width].\n",
    "    \"\"\"\n",
    "    batch_size, num_tiles, height, width = masks.shape\n",
    "    masks_rgb = torch.zeros((batch_size, num_tiles, 3, height, width), dtype=torch.uint8).to(device)\n",
    "\n",
    "    for i, color in enumerate(colors):\n",
    "        color_tensor = torch.tensor(color, dtype=torch.uint8).view(1, 1, 3, 1, 1).to(device)\n",
    "        masks_rgb += (masks == i).unsqueeze(2) * color_tensor\n",
    "\n",
    "    return masks_rgb\n",
    "    \n",
    "def visualize_predictions(true_masks, pred_masks, dims=(256, 256), images_to_visualize=3, batch_size=1):\n",
    "    '''\n",
    "    Visualize the predictions of a model.\n",
    "\n",
    "    Args:\n",
    "        true_masks (torch.Tensor): Ground truth masks of shape [batch_size * num_tiles, 256, 256].\n",
    "        pred_masks (torch.Tensor): Predicted masks of shape [batch_size * num_tiles, 256, 256].\n",
    "        dims (tuple): Dimensions of the images (default: (256, 256)).\n",
    "        batch_size (int): Batch size of the test loader (default: 1).\n",
    "    '''\n",
    "    # Compute nr of images per row\n",
    "    img_per_col = 4000 // dims[0]\n",
    "    imgs_per_row = 6000 // dims[0]\n",
    "    num_tiles = img_per_col * imgs_per_row\n",
    "\n",
    "    # Clip the number of images to visualize\n",
    "    num_images = images_to_visualize * num_tiles * batch_size\n",
    "    true_masks = true_masks[:num_images]\n",
    "    pred_masks = pred_masks[:num_images]\n",
    "    \n",
    "    # Resizing the masks to dims\n",
    "    true_masks = F.interpolate(true_masks.unsqueeze(1).float(), size=dims, mode='nearest').squeeze(1).to(torch.uint8)\n",
    "    pred_masks = F.interpolate(pred_masks.unsqueeze(1).float(), size=dims, mode='nearest').squeeze(1).to(torch.uint8)\n",
    "\n",
    "    # Reshape masks for batching\n",
    "    # [batch_size * num_tiles, 256, 256] -> [batch_size, num_tiles, dims[0], dims[1]]\n",
    "    true_masks = true_masks.view(batch_size, -1, *true_masks.shape[1:])\n",
    "    pred_masks = pred_masks.view(batch_size, -1, *pred_masks.shape[1:])\n",
    "\n",
    "    # Convert masks to RGB\n",
    "    true_masks_rgb = convert_to_rgb(true_masks, colors)\n",
    "    pred_masks_rgb = convert_to_rgb(pred_masks, colors)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Create a grid of predictions\n",
    "        pred_grid = torchvision.utils.make_grid(pred_masks_rgb[i], nrow=imgs_per_row, normalize=False, pad_value=1)\n",
    "        # Create grid of true masks\n",
    "        true_grid = torchvision.utils.make_grid(true_masks_rgb[i], nrow=imgs_per_row, normalize=False, pad_value=1)\n",
    "        \n",
    "        # Display the grids side by side\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
    "        \n",
    "        # Display the true masks grid\n",
    "        axes[0].imshow(true_grid.permute(1, 2, 0).cpu().numpy())\n",
    "        axes[0].axis('off')\n",
    "        axes[0].set_title('True Masks')\n",
    "        \n",
    "        # Display the predicted masks grid\n",
    "        axes[1].imshow(pred_grid.permute(1, 2, 0).cpu().numpy())\n",
    "        axes[1].axis('off')\n",
    "        axes[1].set_title('Predicted Masks')\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "all_y_true_flattened, all_y_pred_flattened = test(test_loader, model, criterion)\n",
    "visualize_predictions(all_y_true_flattened, all_y_pred_flattened, dims=(tiles_dim, tiles_dim), batch_size=batch_size_valtest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv3-12-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
