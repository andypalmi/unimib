{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import albumentations as A\n",
    "from tqdm.auto import tqdm\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 2)\n",
      "['data/original_images/000.jpg' 'data/label_images_semantic/000.png']\n"
     ]
    }
   ],
   "source": [
    "# a list to collect paths of images\n",
    "images_path = []\n",
    "labels_path = []\n",
    "masks_path = []\n",
    "\n",
    "# Get the paths of the images and sort them\n",
    "images_path = sorted(glob.glob('data/original_images/*.jpg'))\n",
    "labels_path = sorted(glob.glob('data/label_images_semantic/*.png'))\n",
    "rgb_masks_path = sorted(glob.glob('data/RGB_color_image_masks/*.png'))\n",
    "\n",
    "paths = np.column_stack((images_path, labels_path))\n",
    "print(paths.shape)\n",
    "print(paths[0])\n",
    "\n",
    "# Apply 80-10-10 split\n",
    "train_split, valtest_split = train_test_split(paths, test_size=0.2, random_state=69420)\n",
    "val_split, test_split = train_test_split(valtest_split, test_size=0.5, random_state=69420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for img in tqdm(paths[:, 0]):\n",
    "#     shape = cv2.imread(img).shape\n",
    "#     print(f'shape for img {img} is {shape}')\n",
    "#     if shape != (4000, 6000, 3):\n",
    "#         print(f'wrong shape for img {img}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>r</th>\n",
       "      <th>g</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unlabeled</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>paved-area</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dirt</td>\n",
       "      <td>130</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>grass</td>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gravel</td>\n",
       "      <td>112</td>\n",
       "      <td>103</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>water</td>\n",
       "      <td>28</td>\n",
       "      <td>42</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rocks</td>\n",
       "      <td>48</td>\n",
       "      <td>41</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pool</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>vegetation</td>\n",
       "      <td>107</td>\n",
       "      <td>142</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>roof</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>wall</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>window</td>\n",
       "      <td>254</td>\n",
       "      <td>228</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>door</td>\n",
       "      <td>254</td>\n",
       "      <td>148</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>fence</td>\n",
       "      <td>190</td>\n",
       "      <td>153</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>fence-pole</td>\n",
       "      <td>153</td>\n",
       "      <td>153</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>person</td>\n",
       "      <td>255</td>\n",
       "      <td>22</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>dog</td>\n",
       "      <td>102</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>car</td>\n",
       "      <td>9</td>\n",
       "      <td>143</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bicycle</td>\n",
       "      <td>119</td>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>tree</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>bald-tree</td>\n",
       "      <td>190</td>\n",
       "      <td>250</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ar-marker</td>\n",
       "      <td>112</td>\n",
       "      <td>150</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>obstacle</td>\n",
       "      <td>2</td>\n",
       "      <td>135</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>conflicting</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           name    r    g    b\n",
       "0     unlabeled    0    0    0\n",
       "1    paved-area  128   64  128\n",
       "2          dirt  130   76    0\n",
       "3         grass    0  102    0\n",
       "4        gravel  112  103   87\n",
       "5         water   28   42  168\n",
       "6         rocks   48   41   30\n",
       "7          pool    0   50   89\n",
       "8    vegetation  107  142   35\n",
       "9          roof   70   70   70\n",
       "10         wall  102  102  156\n",
       "11       window  254  228   12\n",
       "12         door  254  148   12\n",
       "13        fence  190  153  153\n",
       "14   fence-pole  153  153  153\n",
       "15       person  255   22   96\n",
       "16          dog  102   51    0\n",
       "17          car    9  143  150\n",
       "18      bicycle  119   11   32\n",
       "19         tree   51   51    0\n",
       "20    bald-tree  190  250  190\n",
       "21    ar-marker  112  150  146\n",
       "22     obstacle    2  135  115\n",
       "23  conflicting  255    0    0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read number of classes\n",
    "classes_df = pd.read_csv('data/class_dict_seg.csv')\n",
    "classes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if there are any conflicting labels in the masks\n",
    "# for img in tqdm(paths[:, 1]):\n",
    "#     mask = cv2.imread(img, cv2.IMREAD_GRAYSCALE)\n",
    "#     unlabelled = np.sum(mask == 23)\n",
    "#     if unlabelled > 0:\n",
    "#         print(f'Unlabelled pixels in img {img}: {unlabelled} ({unlabelled / mask.size * 100:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 23\n"
     ]
    }
   ],
   "source": [
    "# No conflicting labels found\n",
    "# Therefore there are 23 classes in the dataset\n",
    "nr_classes = len(classes_df) - 1\n",
    "print(f'Number of classes: {nr_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TilesDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None, tiles=True, tiles_dim=512):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "        self.tiles = tiles\n",
    "        self.tiles_dim = tiles_dim\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, mask_path = self.image_paths[idx]\n",
    "        img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            augmented = self.transform(image=img, mask=mask)\n",
    "            img, mask = augmented['image'], augmented['mask']\n",
    "\n",
    "        if self.tiles:\n",
    "            img, mask = self.create_tiles(img, mask, self.tiles_dim)\n",
    "\n",
    "        return img, mask\n",
    "    \n",
    "    def create_tiles(self, img, mask, tiles_dim):\n",
    "        # # Round up to the nearest multiple of tiles_dim\n",
    "        # new_height = (img.shape[1] + tiles_dim - 1) // tiles_dim * tiles_dim\n",
    "        # new_width = (img.shape[2] + tiles_dim - 1) // tiles_dim * tiles_dim\n",
    "        # Round down to the nearest multiple of tiles_dim\n",
    "        new_height = img.shape[1] // tiles_dim * tiles_dim\n",
    "        new_width = img.shape[2] // tiles_dim * tiles_dim\n",
    "        new_shp = (new_height, new_width)\n",
    "\n",
    "        # Resize the image and mask\n",
    "        img = F.interpolate(img.unsqueeze(0), size=new_shp, mode='bilinear', align_corners=False).squeeze(0)\n",
    "        mask = F.interpolate(mask.unsqueeze(0).unsqueeze(0), size=new_shp, mode='nearest').squeeze(0).squeeze(0)\n",
    "        \n",
    "        # Create img tiles and mask tiles\n",
    "        img_tiles = img.unfold(1, tiles_dim, tiles_dim).unfold(2, tiles_dim, tiles_dim)\n",
    "        img_tiles = img_tiles.contiguous().view(3, -1, tiles_dim, tiles_dim)\n",
    "        img_tiles = img_tiles.permute(1, 0, 2, 3)\n",
    "        \n",
    "        mask_tiles = mask.unfold(0, tiles_dim, tiles_dim).unfold(1, tiles_dim, tiles_dim)\n",
    "        mask_tiles = mask_tiles.contiguous().view(-1, tiles_dim, tiles_dim)\n",
    "\n",
    "        # Check if tiles_dim is greater than 512, resize each tile to 512x512\n",
    "        if tiles_dim > 256:\n",
    "            resize_dim = 256\n",
    "            img_tiles = F.interpolate(img_tiles, size=(resize_dim, resize_dim), mode='bilinear', align_corners=False)\n",
    "            mask_tiles = F.interpolate(mask_tiles.unsqueeze(1), size=(resize_dim, resize_dim), mode='nearest').squeeze(1)\n",
    "\n",
    "        return img_tiles, mask_tiles\n",
    "    \n",
    "tiles_dim = 512\n",
    "new_height = (4000 + tiles_dim - 1) // tiles_dim * tiles_dim\n",
    "new_width = (6000 + tiles_dim - 1) // tiles_dim * tiles_dim\n",
    "\n",
    "# Define Albumentations transformations\n",
    "train_transform = A.Compose([\n",
    "    # A.Resize(new_height, new_width, p=1.0),  # Resize the image to the desired shape\n",
    "    A.HorizontalFlip(p=0.5),  # Apply horizontal flip with 50% probability\n",
    "    A.VerticalFlip(p=0.5),  # Apply vertical flip with 50% probability\n",
    "    A.RandomBrightnessContrast(p=0.2),  # Randomly change brightness and contrast\n",
    "    A.OneOf([\n",
    "        A.GaussianBlur(p=1.0),  # Apply Gaussian blur\n",
    "        A.MotionBlur(p=1.0),  # Apply motion blur\n",
    "    ], p=0.2),  # Apply one of the blur operations with 20% probability\n",
    "    A.HueSaturationValue(p=0.2),  # Randomly change hue, saturation, and value\n",
    "    A.RandomGamma(p=0.2),  # Randomly change gamma\n",
    "    A.CLAHE(p=0.2),  # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # Normalize the image\n",
    "    ToTensorV2(),  # Convert image and mask to PyTorch tensors\n",
    "])\n",
    "\n",
    "valtest_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),  # Apply horizontal flip with 50% probability\n",
    "    A.VerticalFlip(p=0.5),  # Apply vertical flip with 50% probability\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # Normalize the image\n",
    "    ToTensorV2(),  # Convert image and mask to PyTorch tensors\n",
    "])\n",
    "\n",
    "# Initialize your custom dataset\n",
    "train_ds = TilesDataset(train_split, transform=train_transform, tiles_dim=tiles_dim)\n",
    "val_ds = TilesDataset(val_split, transform=valtest_transform, tiles_dim=tiles_dim)\n",
    "test_ds = TilesDataset(test_split, transform=valtest_transform, tiles_dim=tiles_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 320, Val dataset length: 40, Test dataset length: 40\n"
     ]
    }
   ],
   "source": [
    "print(f'Train dataset length: {len(train_ds)}, Val dataset length: {len(val_ds)}, Test dataset length: {len(test_ds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader\n",
    "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, pin_memory=False, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, pin_memory=False, num_workers=4)\n",
    "test_loader = DataLoader(test_ds, batch_size=1, shuffle=False, pin_memory=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score, accuracy_score, f1_score\n",
    "\n",
    "def mean_and_per_class_iou(y_true, y_pred, num_classes):\n",
    "    iou_list = []\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = y_pred.flatten()\n",
    "    for cls in range(num_classes):\n",
    "        iou = jaccard_score(y_true == cls, y_pred == cls)\n",
    "        iou_list.append(iou)\n",
    "    return np.mean(iou_list), iou_list\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return accuracy_score(y_true.flatten(), y_pred.flatten())\n",
    "\n",
    "def dice_score(y_true, y_pred, num_classes):\n",
    "    dice_list = []\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = y_pred.flatten()\n",
    "    for cls in range(num_classes):\n",
    "        dice = f1_score(y_true == cls, y_pred == cls)\n",
    "        dice_list.append(dice)\n",
    "    return np.mean(dice_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unet(\n",
       "  (encoder): ResNetEncoder(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (5): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): UnetDecoder(\n",
       "    (center): Identity()\n",
       "    (blocks): ModuleList(\n",
       "      (0): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (segmentation_head): SegmentationHead(\n",
       "    (0): Conv2d(16, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Identity()\n",
       "    (2): Activation(\n",
       "      (activation): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    'arch': 'unet',\n",
    "    'encoder_name': 'resnet34',\n",
    "    'encoder_weights': 'imagenet',\n",
    "    'in_channels': 3,\n",
    "    'classes': nr_classes\n",
    "}\n",
    "\n",
    "model = smp.create_model(**config)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce45df2beeb34d9889878143764e8979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1/100:   0%|          | 0/320 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 3.0284752689301966\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d12c393be60b46c5bdfadd0b1ab4a1c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch 1/100:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# Set up mixed precision training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Accumulation steps (adjust based on GPU memory)\n",
    "accumulation_steps = 4\n",
    "\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 100\n",
    "num_classes = nr_classes\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for i, (imgs, masks) in enumerate(tqdm(train_loader, desc=f'Training Epoch {epoch+1}/{num_epochs}')):\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "\n",
    "        batch_size, num_tiles = imgs.shape[:2]\n",
    "        tiles_batch_size = num_tiles\n",
    "        \n",
    "        # Reshape images: [batch_size, num_tiles, channels, height, width] -> [batch_size * num_tiles, channels, height, width]\n",
    "        imgs = imgs.view(-1, imgs.shape[2], imgs.shape[3], imgs.shape[4])\n",
    "        # Reshape masks: [batch_size, num_tiles, height, width] -> [batch_size * num_tiles, height, width]\n",
    "        masks = masks.view(-1, masks.shape[2], masks.shape[3])\n",
    "\n",
    "        # Convert masks to Long() type\n",
    "        masks = masks.to(torch.long)\n",
    "\n",
    "        # # Due to GPU memory restrictions, I had to process the tiles of each image in batches\n",
    "        # for tile in range(0, num_tiles, tiles_batch_size):\n",
    "            # tiles = imgs[tile:tile+tiles_batch_size]\n",
    "            # masks_tiles = masks[tile:tile+tiles_batch_size]\n",
    "\n",
    "        #     optimizer.zero_grad()\n",
    "\n",
    "        #     with autocast():\n",
    "        #         outputs = model(tiles)\n",
    "        #         loss = criterion(outputs, masks_tiles) / accumulation_steps\n",
    "\n",
    "        #     scaler.scale(loss).backward()\n",
    "\n",
    "        #     if (i + 1) % accumulation_steps == 0:\n",
    "        #         scaler.step(optimizer)\n",
    "        #         scaler.update()\n",
    "        #         optimizer.zero_grad()\n",
    "            \n",
    "        #     train_loss += loss.item() * accumulation_steps  # Adjust for scaled loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, masks) / accumulation_steps\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        train_loss += loss.item() * accumulation_steps  # Adjust for scaled loss\n",
    "\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss/len(train_loader)}')\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks in tqdm(val_loader, desc=f'Validation Epoch {epoch+1}/{num_epochs}'):\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "\n",
    "            batch_size, num_tiles = imgs.shape[:2]\n",
    "            tiles_batch_size = 16\n",
    "            \n",
    "            # Reshape images: [batch_size, num_tiles, channels, height, width] -> [batch_size * num_tiles, channels, height, width]\n",
    "            imgs = imgs.view(-1, imgs.shape[2], imgs.shape[3], imgs.shape[4])\n",
    "            # Reshape masks: [batch_size, num_tiles, height, width] -> [batch_size * num_tiles, height, width]\n",
    "            masks = masks.view(-1, masks.shape[2], masks.shape[3])\n",
    "\n",
    "            # Convert masks to Long() type\n",
    "            masks = masks.to(torch.long)\n",
    "\n",
    "            # # Due to GPU memory restrictions, I had to process the tiles of each image in batches\n",
    "            # for tile in range(0, num_tiles, tiles_batch_size):\n",
    "            #     tiles = imgs[tile:tile+tiles_batch_size]\n",
    "            #     masks_tiles = masks[tile:tile+tiles_batch_size]\n",
    "                \n",
    "            #     with autocast():\n",
    "            #         outputs = model(tiles)\n",
    "            #         loss = criterion(outputs, masks_tiles)\n",
    "            #     val_loss += loss.item()\n",
    "\n",
    "            #     preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            #     all_y_true.append(masks.cpu().numpy())\n",
    "            #     all_y_pred.append(preds)\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, masks)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            all_y_true.append(masks.cpu().numpy())\n",
    "            all_y_pred.append(preds)\n",
    "\n",
    "\n",
    "    all_y_true = np.concatenate(all_y_true, axis=0)\n",
    "    all_y_pred = np.concatenate(all_y_pred, axis=0)\n",
    "\n",
    "    miou, per_class_iou = mean_and_per_class_iou(all_y_true, all_y_pred, num_classes)\n",
    "    acc = accuracy(all_y_true, all_y_pred)\n",
    "    dice = dice_score(all_y_true, all_y_pred, num_classes)\n",
    "\n",
    "    print(f'Validation Loss: {val_loss/len(val_loader)}, Mean IoU: {miou}, Accuracy: {acc}, Dice Score: {dice} \\n per-class IoU{per_class_iou}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv3-12-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
