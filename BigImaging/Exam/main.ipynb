{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, batch 0: 0.0\n",
      "Epoch 0, batch 20: 0.1\n",
      "Epoch 0, batch 40: 0.2\n",
      "Epoch 0, batch 60: 0.3\n",
      "Epoch 0, batch 80: 0.4\n",
      "Epoch 0, batch 100: 0.5\n",
      "Epoch 0, batch 120: 0.6\n",
      "Epoch 0, batch 140: 0.7\n",
      "Epoch 0, batch 160: 0.8\n",
      "Epoch 0, batch 180: 0.9\n",
      "Epoch 0, batch 199: 0.995\n",
      "Epoch 1, batch 0: 1.0\n",
      "Epoch 1, batch 20: 1.1\n",
      "Epoch 1, batch 40: 1.2\n",
      "Epoch 1, batch 60: 1.3\n",
      "Epoch 1, batch 80: 1.4\n",
      "Epoch 1, batch 100: 1.5\n",
      "Epoch 1, batch 120: 1.6\n",
      "Epoch 1, batch 140: 1.7\n",
      "Epoch 1, batch 160: 1.8\n",
      "Epoch 1, batch 180: 1.9\n",
      "Epoch 1, batch 199: 1.995\n",
      "Epoch 2, batch 0: 2.0\n",
      "Epoch 2, batch 20: 2.1\n",
      "Epoch 2, batch 40: 2.2\n",
      "Epoch 2, batch 60: 2.3\n",
      "Epoch 2, batch 80: 2.4\n",
      "Epoch 2, batch 100: 2.5\n",
      "Epoch 2, batch 120: 2.6\n",
      "Epoch 2, batch 140: 2.7\n",
      "Epoch 2, batch 160: 2.8\n",
      "Epoch 2, batch 180: 2.9\n",
      "Epoch 2, batch 199: 2.995\n",
      "Epoch 3, batch 0: 3.0\n",
      "Epoch 3, batch 20: 3.1\n",
      "Epoch 3, batch 40: 3.2\n",
      "Epoch 3, batch 60: 3.3\n",
      "Epoch 3, batch 80: 3.4\n",
      "Epoch 3, batch 100: 3.5\n",
      "Epoch 3, batch 120: 3.6\n",
      "Epoch 3, batch 140: 3.7\n",
      "Epoch 3, batch 160: 3.8\n",
      "Epoch 3, batch 180: 3.9\n",
      "Epoch 3, batch 199: 3.995\n",
      "Epoch 4, batch 0: 4.0\n",
      "Epoch 4, batch 20: 4.1\n",
      "Epoch 4, batch 40: 4.2\n",
      "Epoch 4, batch 60: 4.3\n",
      "Epoch 4, batch 80: 4.4\n",
      "Epoch 4, batch 100: 4.5\n",
      "Epoch 4, batch 120: 4.6\n",
      "Epoch 4, batch 140: 4.7\n",
      "Epoch 4, batch 160: 4.8\n",
      "Epoch 4, batch 180: 4.9\n",
      "Epoch 4, batch 199: 4.995\n",
      "Epoch 5, batch 0: 5.0\n",
      "Epoch 5, batch 20: 5.1\n",
      "Epoch 5, batch 40: 5.2\n",
      "Epoch 5, batch 60: 5.3\n",
      "Epoch 5, batch 80: 5.4\n",
      "Epoch 5, batch 100: 5.5\n",
      "Epoch 5, batch 120: 5.6\n",
      "Epoch 5, batch 140: 5.7\n",
      "Epoch 5, batch 160: 5.8\n",
      "Epoch 5, batch 180: 5.9\n",
      "Epoch 5, batch 199: 5.995\n",
      "Epoch 6, batch 0: 6.0\n",
      "Epoch 6, batch 20: 6.1\n",
      "Epoch 6, batch 40: 6.2\n",
      "Epoch 6, batch 60: 6.3\n",
      "Epoch 6, batch 80: 6.4\n",
      "Epoch 6, batch 100: 6.5\n",
      "Epoch 6, batch 120: 6.6\n",
      "Epoch 6, batch 140: 6.7\n",
      "Epoch 6, batch 160: 6.8\n",
      "Epoch 6, batch 180: 6.9\n",
      "Epoch 6, batch 199: 6.995\n",
      "Epoch 7, batch 0: 7.0\n",
      "Epoch 7, batch 20: 7.1\n",
      "Epoch 7, batch 40: 7.2\n",
      "Epoch 7, batch 60: 7.3\n",
      "Epoch 7, batch 80: 7.4\n",
      "Epoch 7, batch 100: 7.5\n",
      "Epoch 7, batch 120: 7.6\n",
      "Epoch 7, batch 140: 7.7\n",
      "Epoch 7, batch 160: 7.8\n",
      "Epoch 7, batch 180: 7.9\n",
      "Epoch 7, batch 199: 7.995\n",
      "Epoch 8, batch 0: 8.0\n",
      "Epoch 8, batch 20: 8.1\n",
      "Epoch 8, batch 40: 8.2\n",
      "Epoch 8, batch 60: 8.3\n",
      "Epoch 8, batch 80: 8.4\n",
      "Epoch 8, batch 100: 8.5\n",
      "Epoch 8, batch 120: 8.6\n",
      "Epoch 8, batch 140: 8.7\n",
      "Epoch 8, batch 160: 8.8\n",
      "Epoch 8, batch 180: 8.9\n",
      "Epoch 8, batch 199: 8.995\n",
      "Epoch 9, batch 0: 9.0\n",
      "Epoch 9, batch 20: 9.1\n",
      "Epoch 9, batch 40: 9.2\n",
      "Epoch 9, batch 60: 9.3\n",
      "Epoch 9, batch 80: 9.4\n",
      "Epoch 9, batch 100: 9.5\n",
      "Epoch 9, batch 120: 9.6\n",
      "Epoch 9, batch 140: 9.7\n",
      "Epoch 9, batch 160: 9.8\n",
      "Epoch 9, batch 180: 9.9\n",
      "Epoch 9, batch 199: 9.995\n"
     ]
    }
   ],
   "source": [
    "train_loader = [0] * 200\n",
    "\n",
    "for epoch in range(10):\n",
    "    for i in range(200):\n",
    "        if i % (200/10) == 0 or i == 200 -1:\n",
    "            print(f'Epoch {epoch}, batch {i}: {(i/200) + epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.14 (you have 1.4.12). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import albumentations as A\n",
    "from tqdm.auto import tqdm\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>background</th>\n",
       "      <th>(0, 0, 0)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>building-flooded</td>\n",
       "      <td>(255, 0, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>building-non-flooded</td>\n",
       "      <td>(180, 120, 120)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>road-flooded</td>\n",
       "      <td>(160, 150, 20)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>road-non-flooded</td>\n",
       "      <td>(140, 140, 140)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>water</td>\n",
       "      <td>(61, 230, 250)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tree</td>\n",
       "      <td>(0, 82, 255)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>vehicle</td>\n",
       "      <td>(255, 0, 245)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pool</td>\n",
       "      <td>(255, 235, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>grass</td>\n",
       "      <td>(4, 250, 7)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             background        (0, 0, 0)\n",
       "0      building-flooded      (255, 0, 0)\n",
       "1  building-non-flooded  (180, 120, 120)\n",
       "2          road-flooded   (160, 150, 20)\n",
       "3      road-non-flooded  (140, 140, 140)\n",
       "4                 water   (61, 230, 250)\n",
       "5                  tree     (0, 82, 255)\n",
       "6               vehicle    (255, 0, 245)\n",
       "7                  pool    (255, 235, 0)\n",
       "8                 grass      (4, 250, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_excel('data/ColorMasks/ColorPalette-Values.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "from utils.utils import read_class_colors\n",
    "\n",
    "labels_colors, colors, num_classes = read_class_colors('data/ColorMasks/ColorPalette-Values.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tiling import create_and_save_tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val\n"
     ]
    }
   ],
   "source": [
    "test = 'data/FloodNet/val'\n",
    "print(test.split('/')[-1].split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10169'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = 'data/FloodNet/val/images/10169.jpg'\n",
    "os.path.basename(test).split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2343, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>img</th>\n",
       "      <th>mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>val</td>\n",
       "      <td>data/FloodNet/val/images/10169.jpg</td>\n",
       "      <td>data/FloodNet/val/masks/10169_lab.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>val</td>\n",
       "      <td>data/FloodNet/val/images/10173.jpg</td>\n",
       "      <td>data/FloodNet/val/masks/10173_lab.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>val</td>\n",
       "      <td>data/FloodNet/val/images/10177.jpg</td>\n",
       "      <td>data/FloodNet/val/masks/10177_lab.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>val</td>\n",
       "      <td>data/FloodNet/val/images/10178.jpg</td>\n",
       "      <td>data/FloodNet/val/masks/10178_lab.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>val</td>\n",
       "      <td>data/FloodNet/val/images/10809.jpg</td>\n",
       "      <td>data/FloodNet/val/masks/10809_lab.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  split                                 img  \\\n",
       "0   val  data/FloodNet/val/images/10169.jpg   \n",
       "1   val  data/FloodNet/val/images/10173.jpg   \n",
       "2   val  data/FloodNet/val/images/10177.jpg   \n",
       "3   val  data/FloodNet/val/images/10178.jpg   \n",
       "4   val  data/FloodNet/val/images/10809.jpg   \n",
       "\n",
       "                                    mask  \n",
       "0  data/FloodNet/val/masks/10169_lab.png  \n",
       "1  data/FloodNet/val/masks/10173_lab.png  \n",
       "2  data/FloodNet/val/masks/10177_lab.png  \n",
       "3  data/FloodNet/val/masks/10178_lab.png  \n",
       "4  data/FloodNet/val/masks/10809_lab.png  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tiling\n",
    "\n",
    "path = 'data/FloodNet'\n",
    "\n",
    "data = []\n",
    "\n",
    "for folder in glob.glob(f'{path}/*'):\n",
    "    images_paths = sorted(glob.glob(f'{folder}/images/*'))\n",
    "    masks_paths = sorted(glob.glob(f'{folder}/masks/*'))\n",
    "    \n",
    "    for img_path, mask_path in zip(images_paths, masks_paths):\n",
    "        data.append({\n",
    "            'split': folder.split('/')[-1].split('/')[-1],\n",
    "            'img': img_path,\n",
    "            'mask': mask_path\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(data, columns=['split', 'img', 'mask'])\n",
    "\n",
    "print(df.shape)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2343/2343 [05:12<00:00,  7.51it/s]\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "already_created = True\n",
    "\n",
    "def process_row(row):\n",
    "    tiles_dim = 1000\n",
    "    final_dim = 256\n",
    "    create_and_save_tiles(split=row['split'], img_path=row['img'], mask_path=row['mask'], \n",
    "                          tiles_dim=tiles_dim, final_dim=final_dim, output_dir=f'data/tiles_{final_dim}')\n",
    "\n",
    "if not already_created:\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        list(tqdm(executor.map(process_row, [df.iloc[i] for i in range(df.shape[0])]), total=df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list to collect paths of images\n",
    "images_path = []\n",
    "labels_path = []\n",
    "masks_path = []\n",
    "\n",
    "# Get the paths of the images and sort them\n",
    "images_path = sorted(glob.glob('data/original_images/*.jpg'))\n",
    "labels_path = sorted(glob.glob('data/label_images_semantic/*.png'))\n",
    "rgb_masks_path = sorted(glob.glob('data/RGB_color_image_masks/*.png'))\n",
    "\n",
    "paths = np.column_stack((images_path, labels_path))\n",
    "print(paths.shape)\n",
    "print(paths[0])\n",
    "\n",
    "# Apply 80-10-10 split\n",
    "train_split, valtest_split = train_test_split(paths, test_size=0.2, random_state=69420)\n",
    "val_split, test_split = train_test_split(valtest_split, test_size=0.5, random_state=69420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for img in tqdm(paths[:, 0]):\n",
    "#     shape = cv2.imread(img).shape\n",
    "#     print(f'shape for img {img} is {shape}')\n",
    "#     if shape != (4000, 6000, 3):\n",
    "#         print(f'wrong shape for img {img}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read number of classes\n",
    "labels_colors = pd.read_csv('data/class_dict_seg.csv')\n",
    "columns = ['class', 'r', 'g', 'b']\n",
    "labels_colors.columns = columns\n",
    "\n",
    "# Extract RGB values\n",
    "labels_colors['RGB'] = labels_colors[['r', 'g', 'b']].apply(tuple, axis=1)\n",
    "colors = labels_colors['RGB'].values\n",
    "labels_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any conflicting labels in the masks\n",
    "for img in tqdm(paths[:, 1]):\n",
    "    mask = cv2.imread(img, cv2.IMREAD_GRAYSCALE)\n",
    "    unlabelled = np.sum(mask == 23)\n",
    "    if unlabelled > 0:\n",
    "        print(f'Conflicting pixels in img {img}: {unlabelled} ({unlabelled / mask.size * 100:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No conflicting labels found\n",
    "# Therefore there are 23 classes in the dataset\n",
    "num_classes = len(labels_colors) - 1\n",
    "print(f'Number of classes: {num_classes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TilesDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None, tiles=True, tiles_dim=512):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "        self.tiles = tiles\n",
    "        self.tiles_dim = tiles_dim\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, mask_path = self.image_paths[idx]\n",
    "        img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            augmented = self.transform(image=img, mask=mask)\n",
    "            img, mask = augmented['image'], augmented['mask']\n",
    "\n",
    "        if self.tiles:\n",
    "            img, mask = self.create_tiles(img, mask, self.tiles_dim)\n",
    "\n",
    "        return img, mask\n",
    "    \n",
    "    def create_tiles(self, img, mask, tiles_dim):\n",
    "        # Check if resizing is necessary\n",
    "        if img.shape[1] % tiles_dim != 0 or img.shape[2] % tiles_dim != 0:\n",
    "            # Round down to the nearest multiple of tiles_dim\n",
    "            new_height = img.shape[1] // tiles_dim * tiles_dim\n",
    "            new_width = img.shape[2] // tiles_dim * tiles_dim\n",
    "            new_shp = (new_height, new_width)\n",
    "\n",
    "            # Resize the image and mask\n",
    "            img = F.interpolate(img.unsqueeze(0), size=new_shp, mode='bilinear', align_corners=False).squeeze(0)\n",
    "            mask = F.interpolate(mask.unsqueeze(0).unsqueeze(0), size=new_shp, mode='nearest').squeeze(0).squeeze(0)\n",
    "\n",
    "            # Print the shape of the image and mask\n",
    "            print(f\"Image shape: {img.shape}\")\n",
    "            print(f\"Mask shape: {mask.shape}\")\n",
    "        \n",
    "        # Create img tiles and mask tiles\n",
    "        img_tiles = img.unfold(1, tiles_dim, tiles_dim).unfold(2, tiles_dim, tiles_dim)\n",
    "        img_tiles = img_tiles.contiguous().view(3, -1, tiles_dim, tiles_dim).permute(1, 0, 2, 3)\n",
    "        \n",
    "        mask_tiles = mask.unfold(0, tiles_dim, tiles_dim).unfold(1, tiles_dim, tiles_dim)\n",
    "        mask_tiles = mask_tiles.contiguous().view(-1, tiles_dim, tiles_dim)\n",
    "\n",
    "        # Resize tiles if necessary\n",
    "        if tiles_dim > 256:\n",
    "            resize_dim = 256\n",
    "            img_tiles = F.interpolate(img_tiles, size=(resize_dim, resize_dim), mode='bilinear', align_corners=False)\n",
    "            mask_tiles = F.interpolate(mask_tiles.unsqueeze(1), size=(resize_dim, resize_dim), mode='nearest').squeeze(1)\n",
    "\n",
    "        return img_tiles, mask_tiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get image and mask paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dim = 256\n",
    "tiles_dim = 512\n",
    "tiles_path = f'data/tiles_{final_dim}/{tiles_dim}x{tiles_dim}'\n",
    "\n",
    "# Get image paths\n",
    "for folder in os.listdir(tiles_path):\n",
    "    if folder == 'images':\n",
    "        img_paths = sorted(glob.glob(f'{tiles_path}/{folder}/*.png'))\n",
    "    elif folder == 'masks':\n",
    "        mask_paths = sorted(glob.glob(f'{tiles_path}/{folder}/*.png'))\n",
    "\n",
    "# Combine image and mask paths\n",
    "image_paths = np.array(list(zip(img_paths, mask_paths)))\n",
    "print(image_paths.shape)\n",
    "print(image_paths[0])\n",
    "\n",
    "# Apply 80-10-10 split\n",
    "train_split, valtest_split = train_test_split(image_paths, test_size=0.2, random_state=69420)\n",
    "val_split, test_split = train_test_split(valtest_split, test_size=0.5, random_state=69420)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Albumentations transformations\n",
    "train_transform = A.Compose([\n",
    "    # A.Resize(new_height, new_width, p=1.0),  # Resize the image to the desired shape\n",
    "    A.HorizontalFlip(p=0.5),  # Apply horizontal flip with 50% probability\n",
    "    A.VerticalFlip(p=0.5),  # Apply vertical flip with 50% probability\n",
    "    A.RandomBrightnessContrast(p=0.2),  # Randomly change brightness and contrast\n",
    "    A.OneOf([\n",
    "        A.GaussianBlur(p=1.0),  # Apply Gaussian blur\n",
    "        A.MotionBlur(p=1.0),  # Apply motion blur\n",
    "    ], p=0.2),  # Apply one of the blur operations with 20% probability\n",
    "    A.HueSaturationValue(p=0.2),  # Randomly change hue, saturation, and value\n",
    "    A.RandomGamma(p=0.2),  # Randomly change gamma\n",
    "    A.CLAHE(p=0.2),  # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # Normalize the image\n",
    "    ToTensorV2(),  # Convert image and mask to PyTorch tensors\n",
    "])\n",
    "\n",
    "valtest_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),  # Apply horizontal flip with 50% probability\n",
    "    A.VerticalFlip(p=0.5),  # Apply vertical flip with 50% probability\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # Normalize the image\n",
    "    ToTensorV2(),  # Convert image and mask to PyTorch tensors\n",
    "])\n",
    "\n",
    "# Initialize your custom dataset\n",
    "train_ds = TilesDataset(train_split, transform=train_transform, tiles_dim=tiles_dim, tiles=False)\n",
    "val_ds = TilesDataset(val_split, transform=valtest_transform, tiles_dim=tiles_dim, tiles=False)\n",
    "test_ds = TilesDataset(test_split, transform=valtest_transform, tiles_dim=tiles_dim, tiles=False)\n",
    "\n",
    "print(f'Train dataset length: {len(train_ds)}, Val dataset length: {len(val_ds)}, Test dataset length: {len(test_ds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader\n",
    "num_workers = 12\n",
    "batch_size_train = 75\n",
    "batch_size_valtest = 75\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size_train, shuffle=True, pin_memory=False, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size_valtest, shuffle=False, pin_memory=False, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size_valtest, shuffle=False, pin_memory=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_torch(y_true, y_pred, num_classes):\n",
    "    # Flatten the arrays for metric computation\n",
    "    y_true_flat = y_true.view(-1)\n",
    "    y_pred_flat = y_pred.view(-1)\n",
    "\n",
    "    # Compute overall accuracy\n",
    "    acc = (y_true_flat == y_pred_flat).float().mean().item()\n",
    "\n",
    "    # Helper function to compute IoU for a single class\n",
    "    def compute_iou(cls):\n",
    "        intersection = ((y_true_flat == cls) & (y_pred_flat == cls)).float().sum().item()\n",
    "        union = ((y_true_flat == cls) | (y_pred_flat == cls)).float().sum().item()\n",
    "        return intersection / union if union != 0 else 0\n",
    "\n",
    "    # Helper function to compute Dice score for a single class\n",
    "    def compute_dice(cls):\n",
    "        intersection = 2 * ((y_true_flat == cls) & (y_pred_flat == cls)).float().sum().item()\n",
    "        total = (y_true_flat == cls).float().sum().item() + (y_pred_flat == cls).float().sum().item()\n",
    "        return intersection / total if total != 0 else 0\n",
    "\n",
    "    # Compute IoU\n",
    "    iou_list = [compute_iou(cls) for cls in range(num_classes)]\n",
    "    mean_iou = np.mean(iou_list)\n",
    "\n",
    "    # Compute Dice\n",
    "    dice_list = [compute_dice(cls) for cls in range(num_classes)]\n",
    "    mean_dice = np.mean(dice_list)\n",
    "\n",
    "    # Return the metrics\n",
    "    return {\n",
    "        'mean_iou': mean_iou,\n",
    "        'per_class_iou': iou_list,\n",
    "        'accuracy': acc,\n",
    "        'mean_dice': mean_dice,\n",
    "        'per_class_dice': dice_list\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'arch': 'unet',\n",
    "    'encoder_name': 'resnet34',\n",
    "    'encoder_weights': 'imagenet',\n",
    "    'in_channels': 3,\n",
    "    'classes': num_classes\n",
    "}\n",
    "\n",
    "model = smp.create_model(**config)\n",
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "\n",
    "# Create a TensorBoard callback\n",
    "logs_dir = f'logs/{config[\"arch\"]}/{config[\"encoder_name\"]}/tiles_{final_dim}/{tiles_dim}x{tiles_dim}/{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}'\n",
    "os.makedirs(logs_dir, exist_ok=True)  # Ensure the logs directory exists\n",
    "# Get full path to the logs directory\n",
    "logs_dir = os.path.abspath(logs_dir)\n",
    "print(f\"TensorBoard logs directory: {logs_dir}\")\n",
    "\n",
    "writer = SummaryWriter(log_dir=logs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.amp.grad_scaler import GradScaler\n",
    "from torch.amp.autocast_mode import autocast\n",
    "\n",
    "def reshape_imgs_masks(imgs, masks):\n",
    "    imgs, masks = imgs.to(device), masks.to(device)\n",
    "\n",
    "    # Reshape images: [batch_size, num_tiles, channels, height, width] -> [batch_size * num_tiles, channels, height, width]\n",
    "    imgs = imgs.view(-1, imgs.shape[2], imgs.shape[3], imgs.shape[4])\n",
    "    # Reshape masks: [batch_size, num_tiles, height, width] -> [batch_size * num_tiles, height, width]\n",
    "    masks = masks.view(-1, masks.shape[2], masks.shape[3])\n",
    "\n",
    "    # Convert masks to Long() type\n",
    "    masks = masks.to(torch.long)\n",
    "\n",
    "    return imgs,masks\n",
    "\n",
    "def train(train_loss, imgs, masks, scaler, optimizer, criterion, iteration, accumulation_steps=1, use_amp=True, tiles=False):\n",
    "    \n",
    "    # Only reshape images and masks if tiles are being computed by the Dataset class\n",
    "    # Else the source is the already tiled images and masks\n",
    "    if tiles:\n",
    "        imgs, masks = reshape_imgs_masks(imgs, masks)\n",
    "    else:\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        masks = masks.to(torch.long)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if use_amp:\n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, masks) / accumulation_steps\n",
    "        scaler.scale(loss).backward()\n",
    "    else:\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, masks) / accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "    if (iteration + 1) % accumulation_steps == 0:\n",
    "        if use_amp:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    train_loss += loss.detach() * accumulation_steps  # Adjust for scaled loss\n",
    "\n",
    "    if (iteration + 1) % 64 == 0:\n",
    "        print(f'Train loss at iteration {iteration + 1}: {train_loss.item() / iteration :.3f}')\n",
    "\n",
    "    return train_loss\n",
    "\n",
    "def validate(val_loss, imgs, masks, criterion, use_amp=True, tiles=False):\n",
    "    with torch.no_grad():\n",
    "        if tiles:\n",
    "            imgs, masks = reshape_imgs_masks(imgs, masks)\n",
    "        else:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            masks = masks.to(torch.long)\n",
    "\n",
    "        if use_amp:\n",
    "            with autocast(device_type='cuda'):\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.detach()\n",
    "        else:\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, masks)\n",
    "            val_loss += loss.detach()\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "    return val_loss, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up mixed precision training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Accumulation steps (adjust based on GPU memory)\n",
    "accumulation_steps = 1\n",
    "\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    if epoch == 0:\n",
    "        with torch.profiler.profile(\n",
    "            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "            schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),\n",
    "            on_trace_ready=torch.profiler.tensorboard_trace_handler(logs_dir),\n",
    "            record_shapes=True, profile_memory=True, with_stack=True\n",
    "        ) as prof:\n",
    "            for i, (imgs, masks) in enumerate(tqdm(train_loader, desc=f'Training Epoch {epoch+1}/{num_epochs}')):\n",
    "                train_loss = train(train_loss, imgs, masks, scaler, optimizer, criterion, i, accumulation_steps, use_amp=True)\n",
    "                # Profile each 2 batches\n",
    "                if i % 2 == 0:\n",
    "                    prof.step()\n",
    "    else:\n",
    "        for i, (imgs, masks) in enumerate(tqdm(train_loader, desc=f'Training Epoch {epoch+1}/{num_epochs}')):\n",
    "            train_loss = train(train_loss, imgs, masks, scaler, optimizer, criterion, i, accumulation_steps, use_amp=True)\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks in tqdm(val_loader, desc=f'Validation Epoch {epoch+1}/{num_epochs}'):\n",
    "            val_loss, preds = validate(val_loss, imgs, masks, criterion, use_amp=True)\n",
    "            all_y_true.append(masks.to(device))\n",
    "            all_y_pred.append(preds)\n",
    "\n",
    "    all_y_true = torch.cat(all_y_true, dim=0)\n",
    "    all_y_pred = torch.cat(all_y_pred, dim=0)\n",
    "\n",
    "    metrics = compute_metrics_torch(all_y_true, all_y_pred, num_classes)\n",
    "\n",
    "    print(f'Validation Loss: {val_loss.item()/len(val_loader):.3f}, Mean IoU: {metrics[\"mean_iou\"]:.3f}, '\n",
    "      f'Accuracy: {metrics[\"accuracy\"]:.3f}, Dice Score: {metrics[\"mean_dice\"]:.3f}, '\n",
    "      f'per-class IoU: {[f\"Class {i}: {iou:.3f}\" for i, iou in enumerate(metrics[\"per_class_iou\"])]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def test(testloader, model, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, masks in tqdm(testloader, desc=f'Test'):\n",
    "            imgs, masks = reshape_imgs_masks(imgs, masks)\n",
    "\n",
    "            with autocast(device_type='cuda'):\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, masks)\n",
    "                test_loss += loss.detach()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_y_true.append(masks)\n",
    "            all_y_pred.append(preds)\n",
    "\n",
    "    all_y_true_flattened = torch.cat(all_y_true, dim=0)\n",
    "    all_y_pred_flattened = torch.cat(all_y_pred, dim=0)\n",
    "\n",
    "    print(f'All y true shape: {all_y_true_flattened.shape}, All y pred shape: {all_y_pred_flattened.shape}')\n",
    "\n",
    "    metrics = compute_metrics_torch(all_y_true_flattened, all_y_pred_flattened, num_classes)\n",
    "\n",
    "    print(f'Test Loss: {test_loss.item()/len(testloader):.3f}, Mean IoU: {metrics[\"mean_iou\"]:.3f}, '\n",
    "        f'Accuracy: {metrics[\"accuracy\"]:.3f}, Dice Score: {metrics[\"mean_dice\"]:.3f}, '\n",
    "        f'per-class IoU: {[f\"Class {i}: {iou:.3f}\" for i, iou in enumerate(metrics[\"per_class_iou\"])]}')\n",
    "    \n",
    "    return all_y_true_flattened, all_y_pred_flattened\n",
    "\n",
    "def convert_to_rgb(masks, colors):\n",
    "    \"\"\"\n",
    "    Convert a 4D tensor of masks to a 5D tensor of RGB masks.\n",
    "\n",
    "    Args:\n",
    "        masks (Tensor): A 4D tensor of masks with shape [batch_size, num_tiles, height, width].\n",
    "        colors (Tensor): A tensor of RGB color values for each class.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: A 5D tensor of RGB masks with shape [batch_size, num_tiles, 3, height, width].\n",
    "    \"\"\"\n",
    "    batch_size, num_tiles, height, width = masks.shape\n",
    "    masks_rgb = torch.zeros((batch_size, num_tiles, 3, height, width), dtype=torch.uint8).to(device)\n",
    "\n",
    "    for i, color in enumerate(colors):\n",
    "        color_tensor = torch.tensor(color, dtype=torch.uint8).view(1, 1, 3, 1, 1).to(device)\n",
    "        masks_rgb += (masks == i).unsqueeze(2) * color_tensor\n",
    "\n",
    "    return masks_rgb\n",
    "    \n",
    "def visualize_predictions(true_masks, pred_masks, dims=(256, 256), images_to_visualize=3, batch_size=1):\n",
    "    '''\n",
    "    Visualize the predictions of a model.\n",
    "\n",
    "    Args:\n",
    "        true_masks (torch.Tensor): Ground truth masks of shape [batch_size * num_tiles, 256, 256].\n",
    "        pred_masks (torch.Tensor): Predicted masks of shape [batch_size * num_tiles, 256, 256].\n",
    "        dims (tuple): Dimensions of the images (default: (256, 256)).\n",
    "        batch_size (int): Batch size of the test loader (default: 1).\n",
    "    '''\n",
    "    # Compute nr of images per row\n",
    "    img_per_col = 4000 // dims[0]\n",
    "    imgs_per_row = 6000 // dims[0]\n",
    "    num_tiles = img_per_col * imgs_per_row\n",
    "\n",
    "    # Clip the number of images to visualize\n",
    "    num_images = images_to_visualize * num_tiles * batch_size\n",
    "    true_masks = true_masks[:num_images]\n",
    "    pred_masks = pred_masks[:num_images]\n",
    "    \n",
    "    # Resizing the masks to dims\n",
    "    true_masks = F.interpolate(true_masks.unsqueeze(1).float(), size=dims, mode='nearest').squeeze(1).to(torch.uint8)\n",
    "    pred_masks = F.interpolate(pred_masks.unsqueeze(1).float(), size=dims, mode='nearest').squeeze(1).to(torch.uint8)\n",
    "\n",
    "    # Reshape masks for batching\n",
    "    # [batch_size * num_tiles, 256, 256] -> [batch_size, num_tiles, dims[0], dims[1]]\n",
    "    true_masks = true_masks.view(batch_size, -1, *true_masks.shape[1:])\n",
    "    pred_masks = pred_masks.view(batch_size, -1, *pred_masks.shape[1:])\n",
    "\n",
    "    # Convert masks to RGB\n",
    "    true_masks_rgb = convert_to_rgb(true_masks, colors)\n",
    "    pred_masks_rgb = convert_to_rgb(pred_masks, colors)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Create a grid of predictions\n",
    "        pred_grid = torchvision.utils.make_grid(pred_masks_rgb[i], nrow=imgs_per_row, normalize=False, pad_value=1)\n",
    "        # Create grid of true masks\n",
    "        true_grid = torchvision.utils.make_grid(true_masks_rgb[i], nrow=imgs_per_row, normalize=False, pad_value=1)\n",
    "        \n",
    "        # Display the grids side by side\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
    "        \n",
    "        # Display the true masks grid\n",
    "        axes[0].imshow(true_grid.permute(1, 2, 0).cpu().numpy())\n",
    "        axes[0].axis('off')\n",
    "        axes[0].set_title('True Masks')\n",
    "        \n",
    "        # Display the predicted masks grid\n",
    "        axes[1].imshow(pred_grid.permute(1, 2, 0).cpu().numpy())\n",
    "        axes[1].axis('off')\n",
    "        axes[1].set_title('Predicted Masks')\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "all_y_true_flattened, all_y_pred_flattened = test(test_loader, model, criterion)\n",
    "visualize_predictions(all_y_true_flattened, all_y_pred_flattened, dims=(tiles_dim, tiles_dim), batch_size=batch_size_valtest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv3-12-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
