{"cells":[{"cell_type":"markdown","metadata":{"id":"KGQh867e_Xhb"},"source":["# **Lab experience #11 (STUDENTS): Clustering of a dataset with mixed data types**\n","\n","This eleventh lab session aims **to cluster a dataset with mixed data types**. This lab session refers to all Prof. Stella's lectures on clustering.\n","\n","In this lab session, you can **re-use code already developed in the past labs**, but you also need to implement some additional pre-processing steps, before going into the clustering part.\n","\n","The lab session is divided into three main parts:\n","\n","**Part 1**: Dataset loading and exploratory data analysis.\n","\n","**Part 2**: Preprocessing (handling missing values and outliers, scaling, columns dropping if needed, ...).\n","\n","**Part 3**: Clustering. For this part, you are required to **choose at least two clustering methods** to apply (among k-means++, hiearchical clustering, DBSCAN). You can decide to apply two or three of them. Also, you can decide whether it is better to apply them in sequence (see Lab06) or in parallel (i.e., you run them independently and choose the best result). Finally, you need to validate them in an unsupervised manner.\n","\n","_No true labels will be provided this time during the lab._\n"]},{"cell_type":"markdown","source":["## Useful references:\n","\n","- [Manhattan distance](https://www.cs.cornell.edu/courses/JavaAndDS/files/manhattanDistance.pdf)\n","\n","- [Jaccard's similarity and distance](https://en.wikipedia.org/wiki/Jaccard_index)\n","\n","- [Gower's distance: Medium article](https://medium.com/analytics-vidhya/concept-of-gowers-distance-and-it-s-application-using-python-b08cf6139ac2)\n","\n","- [Clustering mixed data types: Medium article](https://medium.com/analytics-vidhya/the-ultimate-guide-for-clustering-mixed-data-1eefa0b4743b)"],"metadata":{"id":"MtLOuK3sHaEv"}},{"cell_type":"markdown","metadata":{"id":"pbBVsr9syzLb"},"source":["## NOTATION:\n","To uniquely identify the number of clusters in the two/three different clustering solutions, please adhere to the following notation:\n","\n","> ```\n","> Kh = number of clusters for the hierarchical clustering solution\n","> Km = number of clusters for the k-means++ clustering solution\n","> Kd = number of clusters for the DBSCAN clustering solution\n","> ```\n","\n","\n","For the labels assigned by the two/three algorithms, please name them as follows:\n","> ```\n","> hierarchical_labels = the labels assigend by the hierarchical clustering solution\n","> kmeans_labels       = the labels assigend by the k-means++ clustering solution\n","> dbscan_labels       = the labels assigend by the DBSCAN clustering solution\n","> ```\n"]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler\n","from sklearn.compose import ColumnTransformer\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler, RobustScaler\n","from yellowbrick.cluster import KElbowVisualizer\n","from yellowbrick.cluster import SilhouetteVisualizer\n","from sklearn.cluster import AgglomerativeClustering\n","from sklearn.cluster import KMeans\n","from sklearn.cluster import DBSCAN"],"metadata":{"id":"Dfz2G2jYk5_T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 1**: Dataset loading and exploratory data analysis"],"metadata":{"id":"lxeCiLWDkZ8G"}},{"cell_type":"markdown","source":["The dataset has attributes of mixed data types.\n","\n","Legend for the attributes:\n","\n","- **ID**: Customer’s unique identifier\n","- **Year_Birth**: Customer's birth year\n","- **Education**: Education Qualification of customer\n","- **Marital_Status**: Marital Status of customer\n","- **Income**: Customer's yearly household income\n","- **Kidhome**: Number of children in customer's household\n","- **Teenhome**: Number of teenagers in customer's household\n","- **Dt_Customer**: Date of customer's enrollment with the company\n","- **Recency**: Number of days since customer's last purchase\n","- **MntWines**: Amount spent on wine\n","- ...\n","\n","\n","Hint:\n","- ```.describe()```: it shows a comprehensive statistical description of the attributes\n","- ```.info()```: it returns the data type for each attribute and the number of non null elements\n","- ```.isna().sum()```: it counts the number of NaN for each attribute\n"],"metadata":{"id":"cFbbLEdtlhvb"}},{"cell_type":"code","source":["# Load the dataset\n","df = pd.read_csv('marketing_campaign.csv', sep='\\t')\n","\n","# Print the first part of the dataset\n","df.head()"],"metadata":{"id":"t7TNcqlhkZjX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compute the basic statistical properties of the dataset. Hint: .describe()\n","#"],"metadata":{"id":"RIFcZv2qnP1R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check the data type for each attribute and the number of non null elements for every one\n","#"],"metadata":{"id":"Jd6c9geBZyVy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Check the values of the categorical (dtype=object) attributes, the presence of _NaN_ values, possible _outliers_, and non-informative attributes (with no variability in the dataset)."],"metadata":{"id":"b9c0kNeUai4j"}},{"cell_type":"code","source":["# Check occurrences of different categories for the categorial features\n","print(df[\"Marital_Status\"].value_counts())\n","print(\"\\n\", df[\"Education\"].value_counts())"],"metadata":{"id":"REjVyQOioUWo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check for NaN values\n","#"],"metadata":{"id":"C4NZ93i4rhza"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 2:** Data preparation"],"metadata":{"id":"Ki3Hch4Eodr5"}},{"cell_type":"markdown","source":["The main objective of this part is to transform the dataset in order to have all attributes as numerical ones (to apply clustering)."],"metadata":{"id":"AlONdKA3Z9fO"}},{"cell_type":"markdown","source":["**Note** the following:\n","\n","- ```Marital_Status``` has a few records associated with unique categories (e.g., \"Alone\", \"YOLO\", \"Absurd\"). Rename them as \"Single\".\n","\n","- ```Year_Birth``` has to be transformed into a numerical value. Then, replace this column with a new ```Age``` column with the corresponding value of the patient's age (reference year is 2024).\n","\n","- ```Education``` and ```Marital_Status``` are categorical features (dtype=object). Then, we need to apply encoding to them.\n","\n","- Reduce the number of columns by shrinking all expenses to a single new column named ```Total_Spent```. To do that, compute the total amount of spendings (any item, i.e., wine, fruits, gold, ...).\n","\n","- ```Dt_Customer``` is type object in the format \"dd-mm-yy\". Hint: .```.to_datetime()``` to transform to a numerical value.\n","\n","- ```Income``` has NaN values. They have to be removed. Hint: ```.dropna()```.\n","\n","- features ```ID```, ```Z_CostContact``` and ```Z_Revenue``` are non-informative. Thus, they can be removed. Hint: ```.drop()```.\n","\n","- remove recording with _outliers_: age can not exceed 100 years, income is most likely below 600000."],"metadata":{"id":"2ABNChvwnZny"}},{"cell_type":"code","source":["# Replace singleton categories with \"Single\"\n","#"],"metadata":{"id":"Y7ft_W8dqbBN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compute the age. Hint: 2024-\"Year_Birth\"\n","#\n","\n","# Replace it in the \"Year-Birth\" column and rename the column as \"Age\"\n","df.rename(..."],"metadata":{"id":"8oIZMdbiYD1G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compute the total amount of spendings (any item) and group it into one column called \"Total_Spent\"\n","#  \"Total_Spent\" = \"MntWines\" + MntFruits + MntMeatProducts + ...\n","df[\"Total_Spent\"] = ..."],"metadata":{"id":"cmSWxEf8YE3J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Transform \"Dt_Customer\" into a numerical attribute,\n","# by computing the number of days each customer engaged to the company (w.r.t. the newest customer). Hint: use .to_datetime(), .dt.days()\n","df['Dt_Customer'] = pd.to_datetime(df.Dt_Customer, dayfirst=True)\n","\n","newest_customer   = df['Dt_Customer'].max()\n","df['newest_customer'] = newest_customer\n","\n","df['days_engaged'] = ...\n","\n","# Drop columns \"Dt_Customer\" and \"newest_customer\" and keep \"days_engaged\"\n","#"],"metadata":{"id":"XnqrkvtCoj-l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Drop NaN values. Hint: use .dropna()\n","#"],"metadata":{"id":"u180GDtKsOkp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dropping redundant features, in this case \"ID\", \"Z_CostContact\", \"Z_Revenue\"\n","#"],"metadata":{"id":"XaE8SDdTnkDh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Outliers/Noise exploration by using sns.pairplot() to plot distributions of three most relevant features\n","# See documentation at https://seaborn.pydata.org/generated/seaborn.pairplot.html\n","plt.figure()\n","sns.pairplot(df[['Income','Age', \"Total_Spent\"]])\n","plt.show()"],"metadata":{"id":"M-2sA8imsYrg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dropping the outliers (simplest way, by thresholding). Hint: observe the plot and read instructions above\n","\n","# outliers in \"Age\"\n","#\n","\n","# outliers in \"Income\"\n","#"],"metadata":{"id":"CZKoHnbAtMMf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check again the new dataset. Hint: .describe(), .info()\n","#\n","#"],"metadata":{"id":"C5ZLiXqYhaM5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# num of samples after cleaning\n","print(len(df))"],"metadata":{"id":"bC6EZPbttShs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 3**: Preprocessing\n","\n","**Encode categorical features**\n","- use OrdinalEncoder for ordinal data\n","- use OneHotEncoder for nominal (unordered) data\n"],"metadata":{"id":"o-BeeOKitfzf"}},{"cell_type":"code","source":["# Encode ordinal features with ordinal encoding method. Hint: use OrdinalEncoder()\n","education_order = ['Basic', '2n Cycle','Graduation','Master', 'PhD']\n","oe              = OrdinalEncoder(...\n","education_oe    = oe.fit_transform(df[['Education']])\n","df_enc          = df.assign(Education_encode = education_oe)      # df_enc is a new dataframe with encoded columns\n","print(df_enc.shape)\n","print(df_enc[['Education', 'Education_encode']])"],"metadata":{"id":"5554TSXFt0an"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Encode nominal features with one-hot encoding method. Hint: use OneHotEncoder()\n","ohe         =  OneHotEncoder(sparse = False, dtype = 'int')\n","Marital_ohe = ohe.fit_transform(...\n","Marital_ohe = pd.DataFrame(data = Marital_ohe, columns = ohe.get_feature_names_out(['Marital_Status']), index = df.index,)"],"metadata":{"id":"J4A1WGrOt48c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Update df_enc with the new column \"Marital_ohe\"\n","df_enc      = ...\n","\n","# Remove non encoded columns (\"Marital_Status\" and \"Education\")\n","df_enc.drop(['Marital_Status','Education'], axis=1, inplace=True)"],"metadata":{"id":"jkj7Hil9N6B9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(20,10))\n","sns.heatmap(df_enc.corr(method='pearson'), cmap='viridis', annot = True, annot_kws={\"size\": 5}, vmin=-1, vmax=1)\n","plt.title('Correlation Matrix')\n","plt.show()"],"metadata":{"id":"Xl6u8-MXlvuU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Note** All features are now numerical. Be careful to use ```df_enc``` now on.\n","\n"],"metadata":{"id":"XLLfrmm0ulh6"}},{"cell_type":"markdown","source":["**Scaling**: Apply z-score (StandardScaler) on continuous variables."],"metadata":{"id":"l9thw3f3j6RI"}},{"cell_type":"code","source":["# binary (dummy) features do not require normalisation\n","binary_columns = ['Marital_Status_Divorced','Marital_Status_Married', 'Marital_Status_Single','Marital_Status_Together','Marital_Status_Widow'\n","                 ,'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1','AcceptedCmp2', 'Complain', 'Response']\n","binary_series = df_enc[binary_columns]"],"metadata":{"id":"HhuHh2xDPcm_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Part of the dataframe needing of scaling\n","df_to_scaler = df_enc.drop(columns=binary_columns)   # excluding binary attributes from df_enc, before scaling\n","\n","#scaling the features\n","scaler = StandardScaler().fit_transform(df_to_scaler)\n","\n","#creating a new dataframe with numerical features scaled\n","scaled_df     = pd.DataFrame(scaler, columns = df_to_scaler.columns)"],"metadata":{"id":"7uUbRONtvu3f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Update the scaled dataframe by including the binary columns\n","scaled_df     = pd.concat([scaled_df.reset_index(drop=True), binary_series.reset_index(drop=True)], axis=1)"],"metadata":{"id":"n2jn5vA-P-vf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot the heatmap of the features pair-wise correlation in the new scaled dataframe. Hint: use sns.heatmap()\n","plt.figure(figsize=(20,10))\n","#\n","plt.title(TITLE_TITLE_TITLE)\n","plt.show()"],"metadata":{"id":"cgAiVVxYtWth"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Note** Be careful to use ```scaled_df``` now on.\n","\n"],"metadata":{"id":"tClCsFvBbXLt"}},{"cell_type":"markdown","source":["# **Part 4:** Clustering\n","\n","- remove mean, column-wise\n","- before going to clustering, apply dimensionality reduction (PCA)\n","- transform the dataframe into numpy to re-use code already developed in previous labs\n","- use the suggested palette"],"metadata":{"id":"4zslCqnrpdEn"}},{"cell_type":"code","source":["# Remove mean column-wise\n"],"metadata":{"id":"KAg1WZsAFp4B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Apply PCA\n","NCOMP = ...\n","pca = PCA(n_components=NCOMP)\n","pca.fit(scaled_df)\n","PCA_df = pd.DataFrame(pca.transform(scaled_df), columns=([\"PC1\",\"PC2\", ADDOTHERCOLUMNSIFNEEDED]))"],"metadata":{"id":"AHIND0e9susE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# To numpy\n","X = PCA_df.to_numpy()"],"metadata":{"id":"6n5PxT1rs1m_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Colors palette for clusters\n","PAL = ['blue', 'green', 'red', 'yellow', 'orange', 'purple', 'magenta', 'cyan', 'brown']"],"metadata":{"id":"slDl55XWsy_n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Note** Be careful to use ```X``` now on."],"metadata":{"id":"lfIOrlKVbd8P"}},{"cell_type":"markdown","source":["#CLUSTERIZE THE DATASET\n","Below, you can find code from previous labs."],"metadata":{"id":"zYh7VZpKINhu"}},{"cell_type":"markdown","source":["**k-means++ clustering**\n","- use the elbow method to choose the best number of clusters (Km)\n","- run k-means++ on the reduced dataset (with NCOMP components)\n","- find the clusters\n","- visualize the solution"],"metadata":{"id":"7p9Bjs4gp7Rm"}},{"cell_type":"code","source":["# Use the elbow method\n","Elbow_M = KElbowVisualizer(KMeans(), k=RANGE_TO_TEST)\n","Elbow_M.fit(X)\n","Elbow_M.show()"],"metadata":{"id":"JV3SEPHrqW96"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Best number of clusters based on the elbow method\n","Km = ..."],"metadata":{"id":"6-Jrxe_YvztU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply k-means++\n","kmeans = KMeans(...\n","\n","print('The final SSE is: %.2f '% kmeans.inertia_)\n","\n","# Scatterplot\n","fig11 = plt.figure('kmeans', figsize=(10,5))\n","# scatterplot here your data objects using the assigned labels and the given palette\n","for k in range(Km):\n","  # complete the following line to plot the cluster centers\n","  plt.scatter(CLUSTER_CENTERS, s=100, color=PAL[k], marker='s', edgecolor='black', linewidth=1.5)\n","sns.set_theme(style='dark')\n","plt.xlabel('PC1')\n","plt.ylabel('PC2')\n","plt.grid()\n","plt.show()"],"metadata":{"id":"AzlcVSgBv6mY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Validation\n","# ----------\n","from sklearn.metrics import silhouette_score\n","\n","# choose the distance metric\n","distance_metric = ..\n","\n","# Compute intra- and inter-cluster distances (dm, Dm). Hint: use the utility function below\n","#\n","\n","# Silhouette score\n","Sm =\n","print(\"With k-means clustering, we found an optimal number of clusters equal to Km=%d with a silhouette score of S=%.3f.\" % (Km, Sm))"],"metadata":{"id":"QJANlrzmwfC-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Hierarchical clustering**"],"metadata":{"id":"gYFhBimGv9jk"}},{"cell_type":"code","source":["# Import useful packages for clustering\n","from scipy.cluster import hierarchy\n","from scipy.cluster.hierarchy import linkage, fcluster\n","from scipy.spatial.distance import pdist as pdist\n","from scipy.spatial.distance import squareform as sf\n","\n","# Choose the main algorithm parameters\n","method_merging =\n","distance_metric =\n","\n","# Apply the algorithm to obtain the hierarchy\n","Z = hierarchy.linkage(X, method_merging, metric=distance_metric, optimal_ordering='true')\n","\n","# Visualize the dendrogram\n","fig21 = plt.figure(figsize=(15, 5))\n","dn = hierarchy.dendrogram(Z, no_plot=0)\n","plt.tick_params(axis='y', which='major', labelsize=15)\n","plt.tick_params(axis='x', which='major', labelsize=8)\n","plt.xlabel('Distance')\n","plt.grid()\n","plt.show()"],"metadata":{"id":"F_tV2Mx_wBX5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cut the forest to have a certain inter-cluster distance (max_d)\n","max_d =\n","\n","# Form the clusters. Note: subtract 1 in order for the labels to start from 0 (as it happens in k-means++)\n","hierarchical_labels =\n","\n","print(hierarchical_labels.shape)\n","print(hierarchical_labels)\n","\n","# Confirm that you cut correctly, to have N clusters\n","Kh = hierarchical_labels.max() + 1\n","print(\"We got %d cluster(s).\" % Kh)\n","\n","\n","# Add a vertical line to the dendrogram indicating the cut\n","plt.figure(fig21)\n","plt.axhline(y=max_d, color='k', linestyle='--')\n","plt.show()\n","\n","# Find clusters centers. Hint: utility function below\n","hierarchical_centers =\n","print(\"\\nWe need to compute %d centroids, as we have %d clusters.\" % (Kh, Kh) )"],"metadata":{"id":"RRoNNUjPyysl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Validation\n","# ----------\n","\n","# Compute intra- and inter-cluster distances (dh, Dh). Hint: use the utility function below\n","#\n","\n","\n","# Silhouette score\n","Sh =\n","print(\"With hierarchical clustering, we found an optimal number of clusters equal to Kh=%d with a silhouette score of Sh=%.3f.\" % (Kh, Sh))\n","\n","\n","\n","# Visualize this clustering solution\n","fig21 = plt.figure('Hierarchical clustering (dendrogram cut at %.2f)' % max_d, figsize=(10,5))\n","# scatterplot here your data objects using the assigned labels and the given palette\n","for k in range(Kh):\n","   # complete the following line to plot the cluster centers\n","   plt.scatter(CLUSTER_CENTERS, s=100, color=PAL[k], marker='s', edgecolor='black', linewidth=1.5)\n","sns.set_theme(style='dark')\n","plt.xlabel('PC1')\n","plt.ylabel('PC2')\n","plt.title(\"For this solution, we got K=%d clusters\" % Kh)\n","plt.grid()\n","plt.show()"],"metadata":{"id":"9D1tzP7ZzCec"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**DBSCAN clustering**"],"metadata":{"id":"sKAfxDl9g7jw"}},{"cell_type":"code","source":["from sklearn.cluster import DBSCAN\n","from sklearn.neighbors import NearestNeighbors as knn\n","!pip install kneed                   # run this line only the first time you run this cell\n","from kneed import KneeLocator\n","\n","\n","\n","# Knee method\n","neighborhood_order =\n","neighborhood_set   = knn(n_neighbors=neighborhood_order).fit(X)\n","distances, indices = neighborhood_set.kneighbors(X)\n","distances          = np.sort(distances[:,neighborhood_order-1], axis=0)\n","i = np.arange(len(distances))\n","knee = KneeLocator(...\n","knee_x = knee.knee\n","knee_y = knee.knee_y\n","\n","\n","\n","fig31 = plt.figure(figsize=(5,5))\n","# plot here the ordered distances\n","plt.xlabel(\"Points\")\n","plt.ylabel(\"Distance\")\n","plt.grid()\n","plt.axvline(x=knee_x, color='k', linestyle='--')\n","plt.axhline(y=knee_y, color='k', linestyle='--')\n","plt.plot((knee_x), (knee_y), 'o', color='r')\n","plt.show()\n","\n","\n","# Apply DBSCAN\n","dbscan = DBSCAN(...\n","dbscan_labels = dbscan.labels_\n","\n","# find the number of clusters formed by the algorithm\n","Kd = ...\n","\n","print(\"We got %d cluster(s).\" % Kd)\n","# print(dbscan_labels)"],"metadata":{"id":"-5UxprkF7nLP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Find clusters centers. Hint: utility function below\n","dbscan_centers = ...\n","print(\"\\nWe need to compute %d centroids, as we have %d clusters.\" % (Kd, Kd) )"],"metadata":{"id":"l32W6iEO2v6Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Validation\n","# ----------\n","\n","# Compute intra- and inter-cluster distances (dd, Dd). Hint: use utility function below\n","#\n","\n","\n","# Silhouette score\n","Sd =\n","print(\"With DBSCAN clustering, we found an optimal number of clusters equal to Kd=%d with a silhouette score of Sd=%.3f.\" % (Kd, Sd))\n","\n","\n","\n","# Visualize this clustering solution\n","fig31 = plt.figure('DBSCAN clustering', figsize=(10,5))\n","# scatterplot\n","for k in range(Kd):\n","   # complete the line below\n","   plt.scatter(CLUSTER_CENTRES, s=100, marker='s', edgecolor='black', linewidth=1.5, color=PAL[k])\n","sns.set_theme(style='dark')\n","plt.xlabel('PC1')\n","plt.ylabel('PC2')\n","plt.title(\"For this solution, we got K=%d clusters\" % Kd)\n","plt.grid()\n","plt.show()"],"metadata":{"id":"7ilb44mD2gS8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Compare clustering solutions"],"metadata":{"id":"MN2946FETag6"}},{"cell_type":"code","source":["from sklearn import metrics\n","y1 = hierarchical_labels   # predicted labels from hierarchical clustering\n","y2 = kmeans.labels_        # predicted labels from k-means clustering\n","y3 = dbscan_labels         # predicted labels from DBSCANs clustering\n","\n","#\n","#\n","#\n","#"],"metadata":{"id":"vYujfq1e0Gk1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Utility functions"],"metadata":{"id":"lNTsU8zfpX_c"}},{"cell_type":"code","source":["# [FROM SOLUTION OF LAB#4] THIS IS A **METHOD** THAT YOU CAN USE IN THE NEXT LAB SESSIONS TO find visualize data in 2D with clusters in different colours\n","\n","def PCA_tSNE_visualization(data2visualize, NCOMP, LABELS, PAL):\n","\n","  '''\n","  INPUT\n","  data2visualize    - data matrix to visualize\n","  NCOMP             - no. of components to decompose the dataset during PCA\n","  LABELS            - labels given by the clustering solution\n","  PAL               - palette of colours to distinguish between clusters\n","  '''\n","\n","  '''\n","  OUTPUT\n","  Two figures: one using PCA and one using tSNE\n","  '''\n","\n","\n","  # PCA\n","  from sklearn.decomposition import PCA\n","  pca = PCA(n_components=NCOMP)\n","  pca_result = pca.fit_transform(data2visualize)\n","  print('PCA: explained variation per principal component: {}'.format(pca.explained_variance_ratio_.round(2)))\n","\n","  # tSNE\n","  from sklearn.manifold import TSNE\n","  print('\\nApplying tSNE...')\n","  np.random.seed(100)\n","  tsne = TSNE(n_components=2, verbose=0, perplexity=20, n_iter=300)\n","  tsne_results = tsne.fit_transform(data2visualize)\n","\n","\n","  # Plots\n","  fig1000 = plt.figure(figsize=(10,5))\n","  fig1000.suptitle('Dimensionality reduction of the dataset', fontsize=16)\n","\n","\n","  # Plot 1: 2D image of the entire dataset\n","  ax1 = fig1000.add_subplot(121)\n","  sns.scatterplot(x=pca_result[:,0], y=pca_result[:,1], ax=ax1, hue=LABELS, palette=PAL)\n","  ax1.set_xlabel('Dimension 1', fontsize=10)\n","  ax1.set_ylabel('Dimension 2', fontsize=10)\n","  ax1.title.set_text('PCA')\n","  plt.grid()\n","\n","  ax2= fig1000.add_subplot(122)\n","  sns.scatterplot(x=tsne_results[:,0], y=tsne_results[:,1], ax=ax2, hue=LABELS, palette=PAL)\n","  ax2.set_xlabel('Dimension 1', fontsize=10)\n","  ax2.set_ylabel('Dimension 2', fontsize=10)\n","  ax2.title.set_text('tSNE')\n","  plt.grid()\n","  plt.show()"],"metadata":{"id":"u8TNT5eppaDI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# [FROM SOLUTION OF LAB#3] THIS IS A **METHOD** THAT YOU CAN USE IN THE NEXT LAB SESSIONS TO compute the intra- and inter-cluster distances\n","\n","def intra_inter_cluster_distances(data, K, labels, cluster_centers, distance_metric):\n","\n","  '''\n","  INPUT\n","  data            - data matrix for which to compute the proximity matrix\n","  K               - the expected number of clusters\n","  labels          - predicted labels from the clustering solution applied to data\n","  cluster_centers - cluster centres from the clustering solution applied to data\n","  distance_metric - metric to compute the distances within and between clusters. Here, you use the same metric for both measurements (but it might be possible to use two different metrics)\n","  '''\n","\n","  '''\n","  OUTPUT\n","  d               - intra-cluster distance\n","  D               - inter-cluster distances\n","  '''\n","\n","  from scipy.spatial.distance import pdist as pdist\n","  from scipy.spatial.distance import squareform as sf\n","\n","\n","  # Intra-cluster distances (average over all pairwise distances) ----------------- NOTE: bug fixed here!\n","  PM = pdist(data, metric=distance_metric)\n","  PM = sf(PM).round(2)\n","\n","  d = np.zeros(K)\n","  for k in range(K):\n","    ind = np.array( np.where(labels == k ) )\n","    for r in range(ind.size):\n","      d[k] = d[k] + np.sum( PM[ [ind[0][r]], [ind] ] )\n","    d[k] = d[k]/2                                          # not to consider pairs of pair-wise distance between objects twice (the PM is symmetric)\n","    d[k] = d[k]/( (ind.size*(ind.size-1)) / 2 )            # to compute the average among N*(N-1)/2 possible unique pairs\n","  print(\"The intra-cluster distance of the clusters are: \", d.round(2))\n","\n","\n","  # Inter-cluster distance ---------------------------------------------------\n","  D = pdist(cluster_centers, metric=distance_metric)\n","  D = sf(D).round(2)\n","  print(\"\\nAll pair-wise inter-cluster distances:\\n\", D)\n","\n","  return d, D"],"metadata":{"id":"dJD1rpZMxSKM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# [FROM SOLUTION OF LAB#2] THIS IS A **METHOD** THAT YOU CAN USE IN THE NEXT LAB SESSIONS TO find cluster centers\n","\n","def find_cluster_centers(data, K, labels):\n","\n","  '''\n","  INPUT\n","  data    - data matrix for which to compute the proximity matrix\n","  K       - the expected number of clusters\n","  labels  - predicted labels from the clustering solution applied to data\n","  '''\n","\n","  '''\n","  OUTPUT\n","  cluster_centers   - cluster centres from the clustering solution applied to data\n","  '''\n","\n","  # Initialize the output\n","  cluster_centers = np.zeros((K, np.shape(data)[1]))   # np.shape(data)[1] = no. of attributes\n","\n","  print(\"%d centroids are being computed, as we have %d clusters.\" % (K, K) )\n","\n","  for k in range(0, K):\n","    ind = np.array( np.where( labels == k ) )\n","    cluster_points = data[ind, :][0]\n","    cluster_centers[k,:] = np.mean(cluster_points, axis=0) # cluster_points.mean(axis=0)\n","    print(\"The centroid of cluster %d has coordinates: \" % (k), *cluster_centers[k,:].round(2))\n","\n","  return cluster_centers"],"metadata":{"id":"5w8oZuXKztBh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A4fbyRSCEPYG"},"source":["# _This it the end of Lab session #11_ ✅\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}