{"cells":[{"cell_type":"markdown","metadata":{"id":"shbM3zJAuPI9"},"source":["In this lab you will do the following steps in order:\n","\n","1. Load and normalizing the CIFAR10 training and test datasets using\n","   ``torchvision``\n","2. Define a Convolution Neural Network\n","3. Define a loss function and optimizer\n","4. Train the network on the training data\n","5. Test the network on the test data\n","\n","Using ``torchvision``, itâ€™s extremely easy to load CIFAR10.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Qf9I_7NdzTf6"},"source":["How to install a different version of a package"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"S4ZhoQ3vuPI_"},"outputs":[],"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","from torchmetrics import Accuracy\n","import numpy as np\n","\n","from torch.utils.tensorboard import SummaryWriter\n","writer = SummaryWriter()"]},{"cell_type":"markdown","metadata":{"id":"6F0vCe0kzcJ9"},"source":["Use GPU if available"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1712606114657,"user":{"displayName":"Mirko Agarla","userId":"02007240672373134675"},"user_tz":-120},"id":"5bun1lQdwoqy","outputId":"9d3e642d-6ae6-46ed-9884-4307b0b90e6c"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"markdown","metadata":{},"source":["0. Hyperparameters"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Hyperparameters\n","batch_size = 256\n","learning_rate = 0.001\n","max_epochs = 100\n","\n","# Early stopping parameters\n","prev_val_acc = -np.inf\n","best_val = None\n","num_epochs_no_improve = 0\n","patience = 10\n","\n","experiment_name = 'lab_05_model1'"]},{"cell_type":"markdown","metadata":{"id":"dZyKEHctuPI_"},"source":["1. Load and normalizing the CIFAR10 training and test datasets using\n","   ``torchvision``\n","   \n","The output of [torchvision datasets](https://pytorch.org/vision/stable/datasets.html#datasets) are PILImage images of range [0, 1].\n","We [transform](https://pytorch.org/vision/stable/transforms.html) them to Tensors of normalized range [-1, 1]. Then we call the [dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"nV92dK3ruPI_"},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]}],"source":["# Fix for SSL certificate issue with torchvision datasets download\n","# @see https://stackoverflow.com/a/49174340\n","import ssl\n","ssl._create_default_https_context = ssl._create_unverified_context\n","\n","# Define data transformation pipeline\n","transform = transforms.Compose([\n","    # Convert PIL images to PyTorch tensors\n","    transforms.ToTensor(),\n","    # Normalize pixel values\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n","\n","classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","num_classes = 10"]},{"cell_type":"markdown","metadata":{"id":"5nB4FDsouPJA"},"source":["Let us [show](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html#matplotlib-pyplot-imshow) some of the training images\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"KVCb2s_QuPJA"},"outputs":[{"name":"stdout","output_type":"stream","text":["Trainloader and trainset size: 196 50000 {'frog': 5000, 'truck': 5000, 'deer': 5000, 'car': 5000, 'bird': 5000, 'horse': 5000, 'ship': 5000, 'cat': 5000, 'dog': 5000, 'plane': 5000}\n","\n","Testloader and testset size: 40 10000 {'cat': 1000, 'ship': 1000, 'plane': 1000, 'frog': 1000, 'car': 1000, 'truck': 1000, 'dog': 1000, 'horse': 1000, 'deer': 1000, 'bird': 1000}\n"]}],"source":["import matplotlib.pyplot as plt  # Import library for plotting\n","import numpy as np  # Import library for numerical computations\n","from collections import Counter  # Import Counter for counting elements\n","\n","# Function to display an image\n","def imshow(image):\n","    mean=torch.tensor([0.485, 0.456, 0.406])\n","    std=torch.tensor([0.229, 0.224, 0.225])\n","\n","    # Unnormalize the image channels to [0, 1]\n","    image = image.mul(std.unsqueeze(1).unsqueeze(2))  # More efficient element-wise multiplication\n","    image = image.add(mean.unsqueeze(1).unsqueeze(2))  # Efficient element-wise addition\n","\n","    image= image.clamp(0, 1)\n","\n","    # Convert the tensor to a NumPy array\n","    npimg = image.numpy()\n","    # Plot the image using matplotlib\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))  # Transpose for correct display\n","\n","# ------------------ Train Loader Section ------------------\n","# Count the frequency of each class in the training set\n","stat = dict(Counter(trainset.targets))\n","\n","# Create a new dictionary with class names as keys\n","new_stat = stat.copy()\n","for k in stat.keys():\n","    new_stat[classes[k]] = stat[k]\n","    del new_stat[k]\n","    \n","print(\"Trainloader and trainset size:\", len(trainloader), len(trainset), new_stat)\n","\n","# # Get a batch of random training images and their labels\n","# dataiter = iter(trainloader)\n","# images, labels = next(dataiter)\n","# print('Image tensor size', images.shape)\n","# imshow(torchvision.utils.make_grid(images))\n","\n","# ------------------ Test Loader Section -----------------\n","# Similar steps for the test loader\n","stat = dict(Counter(testset.targets))\n","new_stat = stat.copy()\n","for k in stat.keys():\n","    new_stat[classes[k]] = stat[k]\n","    del new_stat[k]\n","\n","print(\"\\nTestloader and testset size:\", len(testloader), len(testset), new_stat)"]},{"cell_type":"markdown","metadata":{"id":"FWjfTuThuPJA"},"source":["2. Define a Convolution Neural Network.\n","[network layers](https://pytorch.org/docs/stable/nn.html#convolution-layers)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"8HW6XRf7uPJB"},"outputs":[{"name":"stdout","output_type":"stream","text":["Net(\n","  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (conv1): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv2): Conv2d(6, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv3): Conv2d(12, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv4): Conv2d(18, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv5): Conv2d(24, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv6): Conv2d(30, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (fc1): Linear(in_features=576, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=64, bias=True)\n","  (fc3): Linear(in_features=64, out_features=32, bias=True)\n","  (fc4): Linear(in_features=32, out_features=10, bias=True)\n",")\n"]}],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.pool = nn.MaxPool2d(2, 2)        \n","        self.conv1 = nn.Conv2d(3, 6, 3, padding=1)\n","        self.conv2 = nn.Conv2d(6, 12, 3, padding=1)\n","        self.conv3 = nn.Conv2d(12, 18, 3, padding=1)\n","        self.conv4 = nn.Conv2d(18, 24, 3, padding=1)\n","        self.conv5 = nn.Conv2d(24, 30, 3, padding=1)\n","        self.conv6 = nn.Conv2d(30, 36, 3, padding=1)\n","        self.fc1 = nn.Linear(36 * 4 * 4, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 32)\n","        self.fc4 = nn.Linear(32, 10)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x)) # 32 -> 32\n","        x = self.pool(F.relu(self.conv2(x))) # 32 -> 32 -> 16\n","        x = F.relu(self.conv3(x)) # 16 -> 16\n","        x = self.pool(F.relu(self.conv4(x))) # 16 -> 16 -> 8\n","        x = F.relu(self.conv5(x)) # 8 -> 8\n","        x = self.pool(F.relu(self.conv6(x))) # 8 -> 8 -> 4\n","        x = torch.flatten(x, 1) # flatten all dimensions except batch\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = F.relu(self.fc3(x))\n","        x = self.fc4(x)\n","        return x\n","\n","net = Net()\n","print(net)"]},{"cell_type":"markdown","metadata":{"id":"O2P0Zlbl2Stb"},"source":["Compute the receptive field of the network"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"tsX3Q7q32SgF"},"outputs":[],"source":["# This line attempts to clone a Git repository using a shell command.\n","# !git clone https://github.com/Fangyh09/pytorch-receptive-field.git\n","\n","# This line would move the downloaded directory\n","# !mv -v pytorch-receptive-field/torch_receptive_field ./\n","\n","# Import the 'receptive_field' function from the 'torch_receptive_field' library.\n","# from torch_receptive_field import receptive_field\n","\n","# Calculate the receptive field of the network 'net' for an input image size of\n","# 3 channels (RGB) and 32x32 pixels. The 'receptive_field' function would analyze the network architecture and input size to determine\n","# the receptive field size for each layer and the overall network.\n","# receptive_field(net, input_size=(3, 32, 32))\n"]},{"cell_type":"markdown","metadata":{"id":"hmA4DkACuPJB"},"source":["3. Define a loss function and optimizer\n","\n","Let's use a Classification [Cross-Entropy](https://pytorch.org/docs/stable/nn.html#loss-functions) loss and [SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD) with momentum.\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"yl7S3NpruPJB"},"outputs":[],"source":["import torch.optim as optim\n","criterion = nn.CrossEntropyLoss()\n","# optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n","optimizer = optim.Adam(net.parameters(), lr=learning_rate)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# TorchMetrics for micro and macro accuracy\n","micro_acc = Accuracy(task='multiclass', num_classes=10, average='micro').to(device)\n","macro_acc = Accuracy(task='multiclass', num_classes=10, average='macro').to(device)"]},{"cell_type":"markdown","metadata":{"id":"bmu1-dvfuPJB"},"source":["4. Train the network on the training data\n","\n","\n","We simply have to loop over our data iterator, and feed the inputs to the\n","network and optimize.\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"928O4nWYuPJC"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/100], Loss: 1.9507, Validation Micro Acc: 0.4375, Validation Macro Acc: 0.2778\n","Saved best\n","Epoch [2/100], Loss: 1.5900, Validation Micro Acc: 0.3750, Validation Macro Acc: 0.2407\n","num_epochs_no_improve  1\n","Epoch [3/100], Loss: 1.4625, Validation Micro Acc: 0.3750, Validation Macro Acc: 0.2500\n","num_epochs_no_improve  2\n","Epoch [4/100], Loss: 1.3685, Validation Micro Acc: 0.4375, Validation Macro Acc: 0.3125\n","Saved best\n","Epoch [5/100], Loss: 1.2856, Validation Micro Acc: 0.3750, Validation Macro Acc: 0.2963\n","num_epochs_no_improve  1\n","Epoch [6/100], Loss: 1.2089, Validation Micro Acc: 0.3750, Validation Macro Acc: 0.2963\n","num_epochs_no_improve  2\n","Epoch [7/100], Loss: 1.1329, Validation Micro Acc: 0.5000, Validation Macro Acc: 0.4074\n","Saved best\n","Epoch [8/100], Loss: 1.0734, Validation Micro Acc: 0.5000, Validation Macro Acc: 0.3704\n","num_epochs_no_improve  1\n","Epoch [9/100], Loss: 1.0285, Validation Micro Acc: 0.5625, Validation Macro Acc: 0.4583\n","Saved best\n","Epoch [10/100], Loss: 0.9669, Validation Micro Acc: 0.5625, Validation Macro Acc: 0.3958\n","num_epochs_no_improve  1\n","Epoch [11/100], Loss: 0.9232, Validation Micro Acc: 0.6875, Validation Macro Acc: 0.5625\n","Saved best\n","Epoch [12/100], Loss: 0.8896, Validation Micro Acc: 0.5625, Validation Macro Acc: 0.4259\n","num_epochs_no_improve  1\n","Epoch [13/100], Loss: 0.8395, Validation Micro Acc: 0.5625, Validation Macro Acc: 0.3704\n","num_epochs_no_improve  2\n","Epoch [14/100], Loss: 0.8008, Validation Micro Acc: 0.5625, Validation Macro Acc: 0.3958\n","num_epochs_no_improve  3\n","Epoch [15/100], Loss: 0.7684, Validation Micro Acc: 0.5625, Validation Macro Acc: 0.3958\n","num_epochs_no_improve  4\n","Epoch [16/100], Loss: 0.7328, Validation Micro Acc: 0.6875, Validation Macro Acc: 0.5000\n","num_epochs_no_improve  5\n","Epoch [17/100], Loss: 0.7052, Validation Micro Acc: 0.5625, Validation Macro Acc: 0.3519\n","num_epochs_no_improve  6\n","Epoch [18/100], Loss: 0.6876, Validation Micro Acc: 0.6875, Validation Macro Acc: 0.5185\n","num_epochs_no_improve  7\n","Epoch [19/100], Loss: 0.6510, Validation Micro Acc: 0.5625, Validation Macro Acc: 0.3704\n","num_epochs_no_improve  8\n","Epoch [20/100], Loss: 0.6279, Validation Micro Acc: 0.6875, Validation Macro Acc: 0.5185\n","num_epochs_no_improve  9\n","Epoch [21/100], Loss: 0.6029, Validation Micro Acc: 0.6875, Validation Macro Acc: 0.5833\n","Saved best\n","Epoch [22/100], Loss: 0.5843, Validation Micro Acc: 0.5625, Validation Macro Acc: 0.3704\n","num_epochs_no_improve  1\n","Epoch [23/100], Loss: 0.5609, Validation Micro Acc: 0.6250, Validation Macro Acc: 0.4074\n","num_epochs_no_improve  2\n","Epoch [24/100], Loss: 0.5333, Validation Micro Acc: 0.6875, Validation Macro Acc: 0.5000\n","num_epochs_no_improve  3\n","Epoch [25/100], Loss: 0.5114, Validation Micro Acc: 0.6875, Validation Macro Acc: 0.5185\n","num_epochs_no_improve  4\n","Epoch [26/100], Loss: 0.4946, Validation Micro Acc: 0.7500, Validation Macro Acc: 0.6250\n","Saved best\n","Epoch [27/100], Loss: 0.4834, Validation Micro Acc: 0.7500, Validation Macro Acc: 0.6250\n","num_epochs_no_improve  0\n","Epoch [28/100], Loss: 0.4449, Validation Micro Acc: 0.7500, Validation Macro Acc: 0.6250\n","num_epochs_no_improve  0\n","Epoch [29/100], Loss: 0.4289, Validation Micro Acc: 0.6875, Validation Macro Acc: 0.5833\n","num_epochs_no_improve  1\n","Epoch [30/100], Loss: 0.4190, Validation Micro Acc: 0.5625, Validation Macro Acc: 0.4444\n","num_epochs_no_improve  2\n","Epoch [31/100], Loss: 0.4052, Validation Micro Acc: 0.7500, Validation Macro Acc: 0.5556\n","num_epochs_no_improve  3\n","Epoch [32/100], Loss: 0.3763, Validation Micro Acc: 0.6875, Validation Macro Acc: 0.4667\n","num_epochs_no_improve  4\n","Epoch [33/100], Loss: 0.3697, Validation Micro Acc: 0.7500, Validation Macro Acc: 0.6250\n","num_epochs_no_improve  0\n","Epoch [34/100], Loss: 0.3539, Validation Micro Acc: 0.6875, Validation Macro Acc: 0.4667\n","num_epochs_no_improve  1\n","Epoch [35/100], Loss: 0.3339, Validation Micro Acc: 0.7500, Validation Macro Acc: 0.6250\n","num_epochs_no_improve  0\n","Epoch [36/100], Loss: 0.3174, Validation Micro Acc: 0.5625, Validation Macro Acc: 0.5000\n","num_epochs_no_improve  1\n","Epoch [37/100], Loss: 0.3029, Validation Micro Acc: 0.6250, Validation Macro Acc: 0.4074\n","num_epochs_no_improve  2\n","Epoch [38/100], Loss: 0.3049, Validation Micro Acc: 0.7500, Validation Macro Acc: 0.5556\n","num_epochs_no_improve  3\n","Epoch [39/100], Loss: 0.2780, Validation Micro Acc: 0.6875, Validation Macro Acc: 0.5185\n","num_epochs_no_improve  4\n","Epoch [40/100], Loss: 0.2571, Validation Micro Acc: 0.6875, Validation Macro Acc: 0.4444\n","num_epochs_no_improve  5\n","Epoch [41/100], Loss: 0.2615, Validation Micro Acc: 0.5625, Validation Macro Acc: 0.3704\n","num_epochs_no_improve  6\n","Epoch [42/100], Loss: 0.2460, Validation Micro Acc: 0.6875, Validation Macro Acc: 0.5833\n","num_epochs_no_improve  7\n","Epoch [43/100], Loss: 0.2460, Validation Micro Acc: 0.6875, Validation Macro Acc: 0.5185\n","num_epochs_no_improve  8\n","Epoch [44/100], Loss: 0.2166, Validation Micro Acc: 0.6875, Validation Macro Acc: 0.5185\n","num_epochs_no_improve  9\n","Epoch [45/100], Loss: 0.2198, Validation Micro Acc: 0.6875, Validation Macro Acc: 0.4444\n","Early stopping triggered.\n","Finished Training\n"]}],"source":["net.to(device)\n","\n","for epoch in range(max_epochs):  # loop over the dataset multiple times\n","\n","    # Switch to training mode\n","    net.train()\n","    \n","    running_loss = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","        # get the inputs; data is a list of [inputs, labels] and move to GPU\n","        inputs, labels = data[0].to(device), data[1].to(device)\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","        \n","        # Training loop: forward pass, backward pass, and optimization\n","        # 1. Forward pass:\n","        outputs = net(inputs)  # Pass the input images through the network to get predictions (outputs)\n","        # 2. Calculate loss:\n","        loss = criterion(outputs, labels)  # Compute the loss based on the predictions (outputs) and ground truth labels\n","        # 3. Backward pass:\n","        loss.backward()  # Backpropagate the loss to calculate gradients for each parameter in the network\n","        # 4. Optimization step:\n","        optimizer.step()  # Update the weights and biases of the network based on the calculated gradients\n","\n","        # Compute training accuracy and plot it\n","        train_micro_acc = micro_acc(outputs, labels)\n","        train_macro_acc = macro_acc(outputs, labels)\n","\n","        running_loss += loss.item()\n","\n","    # Switch to evaluation mode on test set\n","    # (actually it's the test set but no parameter optimization is being performed as network is in eval() mode)\n","    net.eval()\n","    \n","    val_running_loss = 0.0\n","    with torch.no_grad():\n","        val_correct = 0\n","        val_total = 0\n","        for data in testloader:\n","            images, labels = data[0].to(device), data[1].to(device)\n","            outputs = net(images)\n","            val_loss = criterion(outputs, labels)\n","            val_running_loss += val_loss.item()\n","            _, predicted = torch.max(outputs, 1)\n","            val_total += labels.size(0)\n","            val_correct += (predicted == labels).sum().item()\n","            val_micro_acc = micro_acc(outputs, labels)\n","            val_macro_acc = macro_acc(outputs, labels)\n","\n","    # Log training loss, validation accuracy to TensorBoard\n","    writer.add_scalar('Loss/train', running_loss / len(trainloader), epoch)\n","    writer.add_scalars('Micro_Accuracy', {'Train': train_micro_acc, 'Val': val_micro_acc}, epoch)\n","    writer.add_scalars('Macro_Accuracy', {'Train': train_macro_acc, 'Val': val_macro_acc}, epoch)\n","\n","    print(f'Epoch [{epoch + 1}/{max_epochs}], Loss: {running_loss / len(trainloader):.4f}, Validation Micro Acc: {val_micro_acc:.4f}, Validation Macro Acc: {val_macro_acc:.4f}')\n","\n","    # Early stopping\n","    if val_macro_acc < prev_val_acc:\n","        num_epochs_no_improve += 1\n","        if num_epochs_no_improve >= patience:\n","            print(\"Early stopping triggered.\")\n","            break\n","    else:\n","        num_epochs_no_improve = 0\n","        prev_val_acc = val_macro_acc\n","\n","    # Check if it is the best model so far and save it\n","    if best_val is None or val_macro_acc > best_val:\n","        best_val = val_macro_acc\n","        data = {\n","            'net': net.state_dict(), # network weights\n","            'opt': optimizer.state_dict(), # optimizer\n","            'epoch': epoch # current epoch number\n","        }\n","        torch.save(data, experiment_name + '_best.pth')\n","        print('Saved best')\n","    else:\n","        print('num_epochs_no_improve ', num_epochs_no_improve)\n","\n","print('Finished Training')\n","writer.close()"]},{"cell_type":"markdown","metadata":{"id":"bzpMI9aUuPJC"},"source":["5. Test the network on the test data\n","\n","\n","We have trained the network for 2 passes over the training dataset.\n","But we need to check if the network has learnt anything at all.\n","\n","We will check this by predicting the class label that the neural network\n","outputs, and checking it against the ground-truth. If the prediction is\n","correct, we add the sample to the list of correct predictions.\n","\n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"e_JnNda2uPJD"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy of the network on the 10000 test images: 71 %\n"]}],"source":["# Load best model\n","checkpoint = torch.load(experiment_name + '_best.pth')\n","net.load_state_dict(checkpoint['net'])\n","\n","# Initialize variables to track accuracy\n","correct = 0\n","total = 0\n","\n","# Disable gradient calculation for better performance during evaluation\n","with torch.no_grad():\n","    # Loop over the test loader\n","    for data in testloader:\n","        \n","        # Get the image and label from the current batch\n","        images, labels = data[0].to(device), data[1].to(device)\n","\n","        # Get the network's prediction for the image\n","        output = net(images)\n","        # smax = torch.nn.Softmax(dim=1)(output.cpu())\n","\n","        # Find the class with the highest probability\n","        _, predicted = torch.max(output, 1)  # Equivalent to pred = torch.argmax(output.cpu(), dim=1)\n","\n","        # Update total number of test images\n","        total += labels.size(0)  # label.size(0) gives the batch size\n","\n","        # Count correct predictions\n","        correct += (predicted == labels).sum().item()  # Count true positives\n","\n","# Calculate and print accuracy\n","print('Accuracy of the network on the 10000 test images: %d %%' % (\n","    100 * correct / total))\n"]},{"cell_type":"markdown","metadata":{"id":"O1Y4HMBOxCE6"},"source":["**!HOMEWORK!**"]},{"cell_type":"markdown","metadata":{"id":"mZW_s0fsB1Xi"},"source":["This homework assignment asks you to performs 2 tasks:\n","\n","1. Analyze Results with Different Network Parameters:\n","\n","This involves training the network with various configurations of network parameters and analyzing the impact on performance. Here's a step-by-step approach:\n","\n","**Choose Network Parameters:**\n","\n","Select the network parameters you want to experiment with. Common choices include:\n","\n","Number of layers: You can try increasing or decreasing the number of layers in your chosen network architecture (e.g., convolutional layers in a CNN).\n","Learning rate: Experiment with different learning rates (e.g., 0.01, 0.001, 0.0001) to find a balance between fast learning and stability.\n","Other parameters: Depending on your network architecture, there might be additional options like:\n","Number of filters in convolutional layers: This affects the complexity of features extracted from the data.\n","Activation functions: Experiment with different activation functions (e.g., ReLU, Leaky ReLU) to introduce non-linearity.\n","Optimizer parameters: Some optimizers (e.g., Adam) have hyperparameters you can adjust.\n","Train the network for a different number of epochs.\n","\n","**Analyze Results:**\n","\n","Compare the performance of the network across different parameter configurations:\n","\n","How accuracy/loss changes with different parameter values.\n","2. Show and Explain Errors of the Best Network:\n","\n","Once you identify the **best performing network configuration** (based on metrics like accuracy or loss), analyze its errors.\n","For example you can generate a confusion matrix. This matrix visualizes how often the network predicted each class correctly or incorrectly.\n","\n","Useful resources:\n","*   [network layers](https://pytorch.org/docs/stable/nn.html#convolution-layers)\n","*   [activation function](https://pytorch.org/docs/stable/nn.html#convolution-layers)\n","*   [loss functions](https://pytorch.org/docs/stable/nn.html#convolution-layers)\n","\n","\n","\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
