{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shbM3zJAuPI9"
      },
      "source": [
        "In this lab you will do the following steps in order:\n",
        "\n",
        "1. Load a new dataset using ``torchvision dataloader``\n",
        "2. Perform transfer learning of a pre-trained NN (Neural Network)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZW_s0fsB1Xi"
      },
      "source": [
        "\n",
        "Useful resources:\n",
        "\n",
        "* [dataloader](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class)\n",
        "*   [network layers](https://pytorch.org/docs/stable/nn.html#convolution-layers)\n",
        "*   [activation function](https://pytorch.org/docs/stable/nn.html#convolution-layers)\n",
        "*   [loss functions](https://pytorch.org/docs/stable/nn.html#convolution-layers)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F0vCe0kzcJ9"
      },
      "source": [
        "Use GPU if available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "5bun1lQdwoqy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import glob  # Import library for finding all files matching a pattern\n",
        "from PIL import Image  # Import library for image processing\n",
        "import numpy as np  # Import library for numerical operations (not used here)\n",
        "import os  # Import library for operating system functionalities\n",
        "from tqdm import tqdm  # Import library for displaying progress bar\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYFtM4aXT0JM"
      },
      "source": [
        "Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "vDRKuw5pGBnW"
      },
      "outputs": [],
      "source": [
        "# #download images\n",
        "# !wget https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/static/imdb_crop.tar\n",
        "# #download metadata\n",
        "# !wget https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/static/imdb_meta.tar\n",
        "# #extract\n",
        "# !tar -xf imdb_crop.tar\n",
        "# !tar -xf imdb_meta.tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # move to data folder\n",
        "# !mv imdb/ data/\n",
        "# !mv imdb_crop/ data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQBhFMSzpFFA"
      },
      "source": [
        "Remove grayscale images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ySjydzAfS1mK"
      },
      "outputs": [],
      "source": [
        "# # Define a path pattern to search for all jpg images within subdirectories of \"data/imdb_crop\"\n",
        "# image_path_pattern = \"data/imdb_crop/*/*.jpg\"\n",
        "\n",
        "# # Find all image file paths matching the pattern\n",
        "# image_paths = glob.glob(image_path_pattern)\n",
        "\n",
        "# imgs_removed = 0\n",
        "# # Iterate through each image path\n",
        "# for image_path in tqdm(image_paths):\n",
        "#   # Open the image using Pillow's Image class\n",
        "#   image = Image.open(image_path)\n",
        "\n",
        "#   # Get the number of color channels in the image (e.g., RGB has 3 channels)\n",
        "#   num_channels = len(image.getbands())\n",
        "\n",
        "#   # Check if the image has a different number of channels than expected (likely grayscale or unsupported format)\n",
        "#   if num_channels != 3:\n",
        "#     # If not 3 channels, remove the image file\n",
        "#     os.remove(image_path)\n",
        "#     # print(f\"Removed {image_path} (not RGB format)\")  # Print statement to show removed files\n",
        "#     imgs_removed += 1\n",
        "\n",
        "# print(f\"Removed {imgs_removed} images\") ### --> Removed 22532 images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEXVvskp2Q4s"
      },
      "source": [
        "Define function to convert numeric date to common date format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "qW-6cXo249Tz"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, timedelta  # Import libraries for date and time manipulation\n",
        "\n",
        "def datenum_to_datetime(datenum):\n",
        "  \"\"\"\n",
        "  Converts a date represented as a floating-point number (Excel-style) to a Python datetime object.\n",
        "\n",
        "  Args:\n",
        "      datenum (float): The date represented as a floating-point number.\n",
        "\n",
        "  Returns:\n",
        "      datetime: The converted datetime object (year only if conversion fails).\n",
        "          If conversion fails due to ValueError, TypeError, or OverflowError,\n",
        "          returns np.nan.\n",
        "  \"\"\"\n",
        "\n",
        "  try:\n",
        "    # Extract components from the datenum\n",
        "    days = datenum % 1  # Extract days (decimal part)\n",
        "    hours = days % 1 * 24  # Extract hours from remaining decimal part\n",
        "    minutes = hours % 1 * 60  # Extract minutes from remaining decimal part\n",
        "    seconds = minutes % 1 * 60  # Extract seconds from remaining decimal part\n",
        "\n",
        "    # Convert to datetime object with separate day, hour, minute, and second components\n",
        "    exact_date = (datetime.fromordinal(int(datenum))  # Convert integer part to date\n",
        "                 + timedelta(days=int(days))  # Add extracted days\n",
        "                 + timedelta(hours=int(hours))  # Add extracted hours\n",
        "                 + timedelta(minutes=int(minutes))  # Add extracted minutes\n",
        "                 + timedelta(seconds=round(seconds)))  # Add extracted seconds (rounded)\n",
        "\n",
        "    # Adjust for Excel's epoch being different from standard epoch (correct for year)\n",
        "    exact_date -= timedelta(days=366)\n",
        "\n",
        "    # Return the year from the converted datetime object\n",
        "    return exact_date.year\n",
        "\n",
        "  except (ValueError, TypeError, OverflowError) as e:\n",
        "    return np.nan  # Return np.nan if conversion fails\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEw-vwCipUeG"
      },
      "source": [
        "Define the [dataloader](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class) class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "8JAnWr6T0d8r"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import torch\n",
        "import collections\n",
        "\n",
        "class FacesDataset(Dataset):\n",
        "      \"\"\"Face Landmarks dataset.\n",
        "\n",
        "      This class loads and preprocesses a dataset of face images with corresponding ages.\n",
        "      It supports train, validation, and test splits.\n",
        "      \"\"\"\n",
        "\n",
        "      def __init__(self, root_dir, transform, split):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (string): Directory containing the images.\n",
        "            transform (callable, optional): Transformation to be applied to the images.\n",
        "            split (string): Split type (\"train\", \"val\", or \"test\").\n",
        "        \"\"\"\n",
        "        self.split=split\n",
        "        self.root_dir = root_dir\n",
        "        self.data = self.get_data()  # Load and preprocess data\n",
        "        total_data_len = int(len(self.data) * 0.5)  # Select small portion of the dataset\n",
        "\n",
        "        # Randomly shuffle indices for train/val/test split\n",
        "        idx = np.arange(total_data_len)\n",
        "        np.random.seed(0)\n",
        "        np.random.shuffle(idx)\n",
        "        print(f\"Shuffled indices (first 5): {idx[:5]}\")  # Print first 5 shuffled indices\n",
        "\n",
        "        # Select data based on split\n",
        "        if split == \"train\":\n",
        "            self.data = self.data[idx[:int(total_data_len * 0.6)]]\n",
        "            print(split, ' indexes ', idx[:int(total_data_len * 0.6)])\n",
        "        elif split == \"val\":\n",
        "            self.data = self.data[idx[int(total_data_len * 0.6):int(total_data_len * 0.8)]]\n",
        "            print(split, ' indexes ', idx[int(total_data_len * 0.6):int(total_data_len * 0.8)])\n",
        "        else:\n",
        "            self.data = self.data[idx[int(total_data_len * 0.8):]]\n",
        "            print(split, ' indexes ', idx[int(total_data_len * 0.8):])\n",
        "\n",
        "        # Analyze age distribution (uncomment to print)\n",
        "        # age_distribution = collections.Counter()\n",
        "        # for i, sample in enumerate(self.data):\n",
        "        #     age_distribution[sample[1]] += 1\n",
        "        # print(age_distribution)  # Uncomment to print the Counter object\n",
        "\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "\n",
        "\n",
        "      def get_data(self):\n",
        "        \"\"\"\n",
        "        Loads and preprocesses data from the IMDB dataset (.MAT file).\n",
        "\n",
        "        This function performs the following steps:\n",
        "            1. Loads data from the MAT file using scipy.io.loadmat.\n",
        "            2. Defines column names for the loaded data.\n",
        "            3. Converts the loaded data into a dictionary.\n",
        "            4. Creates a pandas DataFrame for efficient data manipulation.\n",
        "            5. Prints DataFrame shape and the first few rows (before processing).\n",
        "            6. Converts date of birth to age using vectorized operations.\n",
        "            7. Filters images based on face score and presence of a single face.\n",
        "                - Removes images without a face (face_score != -np.inf).\n",
        "                - Ensures only one face is present (second_face_score.isna()).\n",
        "                - Filters based on minimum face score threshold (face_score >= 3.5).\n",
        "                - Filters for valid age range (0 <= age <= 100).\n",
        "                - Converts age to integer and drops unnecessary columns.\n",
        "            8. Constructs full image paths by prepending the root directory.\n",
        "            9. Filters for images with existing paths using vectorized boolean indexing.\n",
        "            10. Prints DataFrame shape and the first few rows (after processing).\n",
        "            11. Returns the preprocessed data as a NumPy array.\n",
        "        \"\"\"\n",
        "\n",
        "        # Load data from MAT file and define column names\n",
        "        mat_imdb = scipy.io.loadmat('data/imdb/imdb.mat')\n",
        "        columns = [\"full_path\", \"dob\", \"photo_taken\", \"second_face_score\", \"face_score\"]\n",
        "\n",
        "        # Convert loaded data into a dictionary\n",
        "        data_dict = {col: mat_imdb['imdb'][0][0][col][0] for col in columns}\n",
        "\n",
        "        # Create pandas DataFrame for efficient data manipulation\n",
        "        df_imdb = pd.DataFrame(data_dict)\n",
        "        if self.split==\"train\":\n",
        "          print(\"Before processing:\")\n",
        "          print(df_imdb.shape)  # Print DataFrame shape\n",
        "          print(df_imdb.head())  # Print the first few rows \n",
        "\n",
        "        # Convert date of birth to age using vectorized operations\n",
        "        df_imdb['date_of_birth'] = df_imdb['dob'].apply(datenum_to_datetime)\n",
        "        df_imdb['age'] = df_imdb['photo_taken'].sub(df_imdb['date_of_birth'])  # Handle potential NaNs\n",
        "\n",
        "        # Filter images based on face score and presence of a single face\n",
        "        df_imdb = df_imdb[df_imdb['face_score'] != -np.inf]  # Remove images without a face\n",
        "        df_imdb = df_imdb[df_imdb['second_face_score'].isna()]  # Ensure only one face is present\n",
        "        df_imdb = df_imdb[df_imdb['face_score'] >= 3.5]  # Filter based on minimum face score threshold\n",
        "        df_imdb = df_imdb[(df_imdb['age'] <= 100) & (df_imdb['age'] >= 0)]  # Filter for valid age range\n",
        "        df_imdb['age'] = df_imdb['age'].apply(lambda x: int(x))  # Convert age to integer\n",
        "        df_imdb = df_imdb.drop(columns=['date_of_birth', 'dob', 'photo_taken', \"second_face_score\", \"face_score\"])  # Remove unnecessary columns\n",
        "\n",
        "        print(self.root_dir)\n",
        "        # Construct full image paths using vectorized operations\n",
        "        df_imdb['full_path'] = self.root_dir+\"/\"+ df_imdb['full_path'].apply(lambda x: x[0])\n",
        "\n",
        "        # Filter for images with existing paths using vectorized boolean indexing\n",
        "        df_imdb = df_imdb[df_imdb['full_path'].apply(os.path.exists)]\n",
        "\n",
        "        if self.split==\"train\":\n",
        "          print(\"After processing:\")\n",
        "          print(df_imdb.shape)  # Print DataFrame shape\n",
        "          print(df_imdb.head())  # Print the first few rows\n",
        "\n",
        "        return df_imdb.to_numpy()  # Return preprocessed data as a NumPy array\n",
        "\n",
        "      def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the length of the dataset (number of samples).\n",
        "\n",
        "        This method overrides the default behavior of `len` for the dataset object.\n",
        "        It simply returns the length of the internal `data` list, which represents\n",
        "        the preprocessed data after loading and filtering.\n",
        "        \"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "      def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieves a sample (image and corresponding age) at a given index.\n",
        "\n",
        "        This method overrides the default behavior of indexing for the dataset object.\n",
        "        It takes an index `idx` and performs the following:\n",
        "            1. Accesses the image name and age at the specified index from `self.data`.\n",
        "            2. Opens the image using `Image.open` with the full path constructed by\n",
        "               combining `self.root_dir` and `img_name`.\n",
        "            3. Applies the defined transformation (`self.transform`) to the image.\n",
        "            4. Normalizes the age by dividing by 100.\n",
        "            5. Creates a dictionary `sample` containing the preprocessed image (`image`)\n",
        "               and the normalized age as a PyTorch tensor (`torch.tensor(age).float()`).\n",
        "            6. Returns the constructed `sample` dictionary.\n",
        "        \"\"\"\n",
        "        img_name, age = self.data[idx]\n",
        "        # image = Image.open(os.path.join(self.root_dir, img_name))\n",
        "        image = Image.open(os.path.join(img_name))\n",
        "        image = self.transform(image)\n",
        "        age = age / 100\n",
        "\n",
        "        sample = {'image': image, 'age': torch.tensor(age).float()}\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build the train/val/test dataloaders\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "pI1xDbt1omPP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set:\n",
            "Before processing:\n",
            "(460723, 5)\n",
            "                                        full_path     dob  photo_taken  \\\n",
            "0   [01/nm0000001_rm124825600_1899-5-10_1968.jpg]  693726         1968   \n",
            "1  [01/nm0000001_rm3343756032_1899-5-10_1970.jpg]  693726         1970   \n",
            "2   [01/nm0000001_rm577153792_1899-5-10_1968.jpg]  693726         1968   \n",
            "3   [01/nm0000001_rm946909184_1899-5-10_1968.jpg]  693726         1968   \n",
            "4   [01/nm0000001_rm980463616_1899-5-10_1968.jpg]  693726         1968   \n",
            "\n",
            "   second_face_score  face_score  \n",
            "0           1.118973    1.459693  \n",
            "1           1.852008    2.543198  \n",
            "2           2.985660    3.455579  \n",
            "3                NaN    1.872117  \n",
            "4                NaN    1.158766  \n",
            "data/imdb_crop\n",
            "After processing:\n",
            "(71828, 2)\n",
            "                                            full_path  age\n",
            "7   data/imdb_crop/02/nm0000002_rm1363385088_1924-...   80\n",
            "15  data/imdb_crop/02/nm0000002_rm2585828096_1924-...   82\n",
            "18  data/imdb_crop/02/nm0000002_rm2769394176_1924-...   82\n",
            "19  data/imdb_crop/02/nm0000002_rm2780403712_1924-...   80\n",
            "23  data/imdb_crop/02/nm0000002_rm2983566080_1924-...   83\n",
            "Shuffled indices (first 5): [24882  6199  1963 27427 30965]\n",
            "train  indexes  [24882  6199  1963 ...  3115 23828 29248]\n",
            "Validation set:\n",
            "data/imdb_crop\n",
            "Shuffled indices (first 5): [24882  6199  1963 27427 30965]\n",
            "val  indexes  [33081 28301  3774 ... 33630 17672 12652]\n",
            "Test set:\n",
            "data/imdb_crop\n",
            "Shuffled indices (first 5): [24882  6199  1963 27427 30965]\n",
            "test  indexes  [ 7956  9855 19755 ... 30403 21243  2732]\n",
            "Number of training samples: 21568\n",
            "Number of validation samples: 7183\n",
            "Number of test samples: 7183\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "# Define data transformations (augmentations for training and normalization)\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),  # Resize images to 256x256\n",
        "    transforms.RandomHorizontalFlip(p=0.5),  # Randomly flip images horizontally for training augmentation\n",
        "    transforms.ToTensor(),  # Convert PIL images to PyTorch tensors\n",
        "    transforms.Normalize(  # Normalize pixel values based on ImageNet statistics\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),  # Resize images to 256x256 (consistent with training)\n",
        "    transforms.ToTensor(),  # Convert PIL images to PyTorch tensors\n",
        "    transforms.Normalize(  # Normalize pixel values using the same statistics\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "# Set batch size\n",
        "bs = 64\n",
        "\n",
        "# Create datasets for training, validation, and testing\n",
        "print(\"Train set:\")\n",
        "trainset = FacesDataset(\"data/imdb_crop\", transform_train, split=\"train\")\n",
        "print(\"Validation set:\")\n",
        "valset = FacesDataset(\"data/imdb_crop\", transform_val, split=\"val\")\n",
        "print(\"Test set:\")\n",
        "testset = FacesDataset(\"data/imdb_crop\", transform_val, split=\"test\")\n",
        "\n",
        "# Create data loaders for efficient batch training and evaluation\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=bs, shuffle=True)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=1, shuffle=False)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Print dataset and dataloader lengths (number of samples and batches)\n",
        "print(f\"Number of training samples: {len(trainloader) * bs}\")\n",
        "print(f\"Number of validation samples: {len(valloader)}\")\n",
        "print(f\"Number of test samples: {len(testloader)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/andrea/unimib/Supervised/00_exercises/lab_06\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data  imdb_crop.tar  imdb_meta.tar  lab_06_CNN_transfer_learning_all.ipynb\n",
            "imdb  imdb_crop\n",
            "imdb.mat\n"
          ]
        }
      ],
      "source": [
        "!pwd\n",
        "!ls\n",
        "!ls data/\n",
        "!ls data/imdb/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCUcnKwBq2qX"
      },
      "source": [
        "2. Define a Neural Network (NN) [Mobilenet](https://pytorch.org/vision/main/models/generated/torchvision.models.mobilenet_v2.html#torchvision.models.mobilenet_v2) pretrained on Imagenet.\n",
        "\n",
        "Replace the last classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "2SoU5x1mq7QG"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MobileNetV2(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2dNormActivation(\n",
              "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU6(inplace=True)\n",
              "    )\n",
              "    (1): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (2): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
              "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (3): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (4): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (5): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (6): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (7): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (8): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (9): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (10): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (11): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (12): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (13): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (14): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (15): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (16): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (17): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (18): Conv2dNormActivation(\n",
              "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU6(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=1280, out_features=512, bias=True)\n",
              "    (1): GELU(approximate='none')\n",
              "    (2): Linear(in_features=512, out_features=32, bias=True)\n",
              "    (3): GELU(approximate='none')\n",
              "    (4): Linear(in_features=32, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torchvision\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the model architecture (MobileNetV2)\n",
        "net = torchvision.models.mobilenet_v2(weights='IMAGENET1K_V1')  # Load pre-trained weights\n",
        "\n",
        "# Adjust the final classification layer\n",
        "num_ftrs = net.classifier[1].in_features  # Get the number of input features for the last layer\n",
        "net.classifier = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 512),  # First linear layer with 512 units\n",
        "    nn.GELU(),  # GELU activation function\n",
        "    nn.Linear(512, 32),  # Second linear layer with 32 units\n",
        "    nn.GELU(),  # GELU activation function\n",
        "    nn.Linear(32, 1)   # Output layer with 1 unit (for age prediction)\n",
        ")\n",
        "\n",
        "# Move the model to the appropriate device (CPU or GPU)\n",
        "net.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "wY0BsgOXddiY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 32, 128, 128]             864\n",
            "       BatchNorm2d-2         [-1, 32, 128, 128]              64\n",
            "             ReLU6-3         [-1, 32, 128, 128]               0\n",
            "            Conv2d-4         [-1, 32, 128, 128]             288\n",
            "       BatchNorm2d-5         [-1, 32, 128, 128]              64\n",
            "             ReLU6-6         [-1, 32, 128, 128]               0\n",
            "            Conv2d-7         [-1, 16, 128, 128]             512\n",
            "       BatchNorm2d-8         [-1, 16, 128, 128]              32\n",
            "  InvertedResidual-9         [-1, 16, 128, 128]               0\n",
            "           Conv2d-10         [-1, 96, 128, 128]           1,536\n",
            "      BatchNorm2d-11         [-1, 96, 128, 128]             192\n",
            "            ReLU6-12         [-1, 96, 128, 128]               0\n",
            "           Conv2d-13           [-1, 96, 64, 64]             864\n",
            "      BatchNorm2d-14           [-1, 96, 64, 64]             192\n",
            "            ReLU6-15           [-1, 96, 64, 64]               0\n",
            "           Conv2d-16           [-1, 24, 64, 64]           2,304\n",
            "      BatchNorm2d-17           [-1, 24, 64, 64]              48\n",
            " InvertedResidual-18           [-1, 24, 64, 64]               0\n",
            "           Conv2d-19          [-1, 144, 64, 64]           3,456\n",
            "      BatchNorm2d-20          [-1, 144, 64, 64]             288\n",
            "            ReLU6-21          [-1, 144, 64, 64]               0\n",
            "           Conv2d-22          [-1, 144, 64, 64]           1,296\n",
            "      BatchNorm2d-23          [-1, 144, 64, 64]             288\n",
            "            ReLU6-24          [-1, 144, 64, 64]               0\n",
            "           Conv2d-25           [-1, 24, 64, 64]           3,456\n",
            "      BatchNorm2d-26           [-1, 24, 64, 64]              48\n",
            " InvertedResidual-27           [-1, 24, 64, 64]               0\n",
            "           Conv2d-28          [-1, 144, 64, 64]           3,456\n",
            "      BatchNorm2d-29          [-1, 144, 64, 64]             288\n",
            "            ReLU6-30          [-1, 144, 64, 64]               0\n",
            "           Conv2d-31          [-1, 144, 32, 32]           1,296\n",
            "      BatchNorm2d-32          [-1, 144, 32, 32]             288\n",
            "            ReLU6-33          [-1, 144, 32, 32]               0\n",
            "           Conv2d-34           [-1, 32, 32, 32]           4,608\n",
            "      BatchNorm2d-35           [-1, 32, 32, 32]              64\n",
            " InvertedResidual-36           [-1, 32, 32, 32]               0\n",
            "           Conv2d-37          [-1, 192, 32, 32]           6,144\n",
            "      BatchNorm2d-38          [-1, 192, 32, 32]             384\n",
            "            ReLU6-39          [-1, 192, 32, 32]               0\n",
            "           Conv2d-40          [-1, 192, 32, 32]           1,728\n",
            "      BatchNorm2d-41          [-1, 192, 32, 32]             384\n",
            "            ReLU6-42          [-1, 192, 32, 32]               0\n",
            "           Conv2d-43           [-1, 32, 32, 32]           6,144\n",
            "      BatchNorm2d-44           [-1, 32, 32, 32]              64\n",
            " InvertedResidual-45           [-1, 32, 32, 32]               0\n",
            "           Conv2d-46          [-1, 192, 32, 32]           6,144\n",
            "      BatchNorm2d-47          [-1, 192, 32, 32]             384\n",
            "            ReLU6-48          [-1, 192, 32, 32]               0\n",
            "           Conv2d-49          [-1, 192, 32, 32]           1,728\n",
            "      BatchNorm2d-50          [-1, 192, 32, 32]             384\n",
            "            ReLU6-51          [-1, 192, 32, 32]               0\n",
            "           Conv2d-52           [-1, 32, 32, 32]           6,144\n",
            "      BatchNorm2d-53           [-1, 32, 32, 32]              64\n",
            " InvertedResidual-54           [-1, 32, 32, 32]               0\n",
            "           Conv2d-55          [-1, 192, 32, 32]           6,144\n",
            "      BatchNorm2d-56          [-1, 192, 32, 32]             384\n",
            "            ReLU6-57          [-1, 192, 32, 32]               0\n",
            "           Conv2d-58          [-1, 192, 16, 16]           1,728\n",
            "      BatchNorm2d-59          [-1, 192, 16, 16]             384\n",
            "            ReLU6-60          [-1, 192, 16, 16]               0\n",
            "           Conv2d-61           [-1, 64, 16, 16]          12,288\n",
            "      BatchNorm2d-62           [-1, 64, 16, 16]             128\n",
            " InvertedResidual-63           [-1, 64, 16, 16]               0\n",
            "           Conv2d-64          [-1, 384, 16, 16]          24,576\n",
            "      BatchNorm2d-65          [-1, 384, 16, 16]             768\n",
            "            ReLU6-66          [-1, 384, 16, 16]               0\n",
            "           Conv2d-67          [-1, 384, 16, 16]           3,456\n",
            "      BatchNorm2d-68          [-1, 384, 16, 16]             768\n",
            "            ReLU6-69          [-1, 384, 16, 16]               0\n",
            "           Conv2d-70           [-1, 64, 16, 16]          24,576\n",
            "      BatchNorm2d-71           [-1, 64, 16, 16]             128\n",
            " InvertedResidual-72           [-1, 64, 16, 16]               0\n",
            "           Conv2d-73          [-1, 384, 16, 16]          24,576\n",
            "      BatchNorm2d-74          [-1, 384, 16, 16]             768\n",
            "            ReLU6-75          [-1, 384, 16, 16]               0\n",
            "           Conv2d-76          [-1, 384, 16, 16]           3,456\n",
            "      BatchNorm2d-77          [-1, 384, 16, 16]             768\n",
            "            ReLU6-78          [-1, 384, 16, 16]               0\n",
            "           Conv2d-79           [-1, 64, 16, 16]          24,576\n",
            "      BatchNorm2d-80           [-1, 64, 16, 16]             128\n",
            " InvertedResidual-81           [-1, 64, 16, 16]               0\n",
            "           Conv2d-82          [-1, 384, 16, 16]          24,576\n",
            "      BatchNorm2d-83          [-1, 384, 16, 16]             768\n",
            "            ReLU6-84          [-1, 384, 16, 16]               0\n",
            "           Conv2d-85          [-1, 384, 16, 16]           3,456\n",
            "      BatchNorm2d-86          [-1, 384, 16, 16]             768\n",
            "            ReLU6-87          [-1, 384, 16, 16]               0\n",
            "           Conv2d-88           [-1, 64, 16, 16]          24,576\n",
            "      BatchNorm2d-89           [-1, 64, 16, 16]             128\n",
            " InvertedResidual-90           [-1, 64, 16, 16]               0\n",
            "           Conv2d-91          [-1, 384, 16, 16]          24,576\n",
            "      BatchNorm2d-92          [-1, 384, 16, 16]             768\n",
            "            ReLU6-93          [-1, 384, 16, 16]               0\n",
            "           Conv2d-94          [-1, 384, 16, 16]           3,456\n",
            "      BatchNorm2d-95          [-1, 384, 16, 16]             768\n",
            "            ReLU6-96          [-1, 384, 16, 16]               0\n",
            "           Conv2d-97           [-1, 96, 16, 16]          36,864\n",
            "      BatchNorm2d-98           [-1, 96, 16, 16]             192\n",
            " InvertedResidual-99           [-1, 96, 16, 16]               0\n",
            "          Conv2d-100          [-1, 576, 16, 16]          55,296\n",
            "     BatchNorm2d-101          [-1, 576, 16, 16]           1,152\n",
            "           ReLU6-102          [-1, 576, 16, 16]               0\n",
            "          Conv2d-103          [-1, 576, 16, 16]           5,184\n",
            "     BatchNorm2d-104          [-1, 576, 16, 16]           1,152\n",
            "           ReLU6-105          [-1, 576, 16, 16]               0\n",
            "          Conv2d-106           [-1, 96, 16, 16]          55,296\n",
            "     BatchNorm2d-107           [-1, 96, 16, 16]             192\n",
            "InvertedResidual-108           [-1, 96, 16, 16]               0\n",
            "          Conv2d-109          [-1, 576, 16, 16]          55,296\n",
            "     BatchNorm2d-110          [-1, 576, 16, 16]           1,152\n",
            "           ReLU6-111          [-1, 576, 16, 16]               0\n",
            "          Conv2d-112          [-1, 576, 16, 16]           5,184\n",
            "     BatchNorm2d-113          [-1, 576, 16, 16]           1,152\n",
            "           ReLU6-114          [-1, 576, 16, 16]               0\n",
            "          Conv2d-115           [-1, 96, 16, 16]          55,296\n",
            "     BatchNorm2d-116           [-1, 96, 16, 16]             192\n",
            "InvertedResidual-117           [-1, 96, 16, 16]               0\n",
            "          Conv2d-118          [-1, 576, 16, 16]          55,296\n",
            "     BatchNorm2d-119          [-1, 576, 16, 16]           1,152\n",
            "           ReLU6-120          [-1, 576, 16, 16]               0\n",
            "          Conv2d-121            [-1, 576, 8, 8]           5,184\n",
            "     BatchNorm2d-122            [-1, 576, 8, 8]           1,152\n",
            "           ReLU6-123            [-1, 576, 8, 8]               0\n",
            "          Conv2d-124            [-1, 160, 8, 8]          92,160\n",
            "     BatchNorm2d-125            [-1, 160, 8, 8]             320\n",
            "InvertedResidual-126            [-1, 160, 8, 8]               0\n",
            "          Conv2d-127            [-1, 960, 8, 8]         153,600\n",
            "     BatchNorm2d-128            [-1, 960, 8, 8]           1,920\n",
            "           ReLU6-129            [-1, 960, 8, 8]               0\n",
            "          Conv2d-130            [-1, 960, 8, 8]           8,640\n",
            "     BatchNorm2d-131            [-1, 960, 8, 8]           1,920\n",
            "           ReLU6-132            [-1, 960, 8, 8]               0\n",
            "          Conv2d-133            [-1, 160, 8, 8]         153,600\n",
            "     BatchNorm2d-134            [-1, 160, 8, 8]             320\n",
            "InvertedResidual-135            [-1, 160, 8, 8]               0\n",
            "          Conv2d-136            [-1, 960, 8, 8]         153,600\n",
            "     BatchNorm2d-137            [-1, 960, 8, 8]           1,920\n",
            "           ReLU6-138            [-1, 960, 8, 8]               0\n",
            "          Conv2d-139            [-1, 960, 8, 8]           8,640\n",
            "     BatchNorm2d-140            [-1, 960, 8, 8]           1,920\n",
            "           ReLU6-141            [-1, 960, 8, 8]               0\n",
            "          Conv2d-142            [-1, 160, 8, 8]         153,600\n",
            "     BatchNorm2d-143            [-1, 160, 8, 8]             320\n",
            "InvertedResidual-144            [-1, 160, 8, 8]               0\n",
            "          Conv2d-145            [-1, 960, 8, 8]         153,600\n",
            "     BatchNorm2d-146            [-1, 960, 8, 8]           1,920\n",
            "           ReLU6-147            [-1, 960, 8, 8]               0\n",
            "          Conv2d-148            [-1, 960, 8, 8]           8,640\n",
            "     BatchNorm2d-149            [-1, 960, 8, 8]           1,920\n",
            "           ReLU6-150            [-1, 960, 8, 8]               0\n",
            "          Conv2d-151            [-1, 320, 8, 8]         307,200\n",
            "     BatchNorm2d-152            [-1, 320, 8, 8]             640\n",
            "InvertedResidual-153            [-1, 320, 8, 8]               0\n",
            "          Conv2d-154           [-1, 1280, 8, 8]         409,600\n",
            "     BatchNorm2d-155           [-1, 1280, 8, 8]           2,560\n",
            "           ReLU6-156           [-1, 1280, 8, 8]               0\n",
            "          Linear-157                  [-1, 512]         655,872\n",
            "            GELU-158                  [-1, 512]               0\n",
            "          Linear-159                   [-1, 32]          16,416\n",
            "            GELU-160                   [-1, 32]               0\n",
            "          Linear-161                    [-1, 1]              33\n",
            "================================================================\n",
            "Total params: 2,896,193\n",
            "Trainable params: 2,896,193\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.75\n",
            "Forward/backward pass size (MB): 199.65\n",
            "Params size (MB): 11.05\n",
            "Estimated Total Size (MB): 211.45\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from torchsummary import summary\n",
        "# Print model summary\n",
        "summary(net, (3, 256, 256))  # Input shape (channels, height, width)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87Y7sujZbMCM"
      },
      "source": [
        "**Transfer learning**\n",
        "\n",
        "Train only the last layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "dEpTEgQta5lC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unfreezing layer: classifier, Parameter shape: torch.Size([512, 1280])\n",
            "Unfreezing layer: classifier, Parameter shape: torch.Size([512])\n",
            "Unfreezing layer: classifier, Parameter shape: torch.Size([32, 512])\n",
            "Unfreezing layer: classifier, Parameter shape: torch.Size([32])\n",
            "Unfreezing layer: classifier, Parameter shape: torch.Size([1, 32])\n",
            "Unfreezing layer: classifier, Parameter shape: torch.Size([1])\n"
          ]
        }
      ],
      "source": [
        "# Freeze pre-trained layers and unfreeze the classifier for fine-tuning\n",
        "for key, value in dict(net.named_children()).items():\n",
        "    if \"classifier\" in key:\n",
        "        for param in value.parameters():\n",
        "            param.requires_grad = True\n",
        "            print(f\"Unfreezing layer: {key}, Parameter shape: {param.shape}\")  # Print unfrozen layers (classifier)\n",
        "    else:\n",
        "        for param in value.parameters():\n",
        "            param.requires_grad = False\n",
        "            # print(param)  # Commented out to avoid printing individual parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmA4DkACuPJB"
      },
      "source": [
        "Define a loss function and optimizer\n",
        "\n",
        "Let's use a Regression [L1Loss](https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html) loss and [ADAM](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam) optimizer. [learning rate scheduler](https://towardsdatascience.com/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863#fad1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "-KpTZQWsbKYk"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim  # Optimization algorithms for training the model\n",
        "import torch.nn.functional as F  # Common loss functions and activation functions\n",
        "from scipy.stats import spearmanr, pearsonr  # Statistical functions for correlation calculation\n",
        "import itertools  # Utility functions for generating combinations\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR  # Learning rate scheduler for training\n",
        "import matplotlib.pyplot as plt  # Plotting library for visualization\n",
        "\n",
        "\n",
        "# Define training parameters (epochs, loss function, optimizer, and scheduler)\n",
        "epochs = 2  # Number of training epochs\n",
        "criterion = nn.L1Loss()  # L1 loss function for regression (mean absolute error)\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)  # Adam optimizer with learning rate 0.001\n",
        "scheduler = CosineAnnealingLR(optimizer,\n",
        "                              T_max=len(trainloader) * epochs,  # Maximum number of iterations for scheduler\n",
        "                              eta_min=1e-5)  # Minimum learning rate for scheduler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmu1-dvfuPJB"
      },
      "source": [
        "**Fine-tuning**\n",
        "\n",
        "Train the network on the training data performing a validation at the end of each epoch. The evaluation is done using [PLCC](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) and [SROCC](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "FBQNNzHjbeQT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1, [33, 337] loss: 0.1310\tPLCC: 0.113\tSROCC: 0.208\tlr: 0.000994\n",
            "1, [66, 337] loss: 0.0853\tPLCC: 0.517\tSROCC: 0.469\tlr: 0.000977\n",
            "1, [99, 337] loss: 0.0860\tPLCC: 0.552\tSROCC: 0.505\tlr: 0.000948\n",
            "1, [132, 337] loss: 0.0869\tPLCC: 0.523\tSROCC: 0.482\tlr: 0.000909\n",
            "1, [165, 337] loss: 0.0842\tPLCC: 0.556\tSROCC: 0.495\tlr: 0.000861\n",
            "1, [198, 337] loss: 0.0829\tPLCC: 0.586\tSROCC: 0.528\tlr: 0.000804\n",
            "1, [231, 337] loss: 0.0823\tPLCC: 0.581\tSROCC: 0.540\tlr: 0.000740\n",
            "1, [264, 337] loss: 0.0812\tPLCC: 0.605\tSROCC: 0.552\tlr: 0.000670\n",
            "1, [297, 337] loss: 0.0806\tPLCC: 0.603\tSROCC: 0.567\tlr: 0.000597\n",
            "1, [330, 337] loss: 0.0778\tPLCC: 0.638\tSROCC: 0.576\tlr: 0.000521\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):  # Loop over the dataset for multiple epochs\n",
        "    running_loss = []  # List to store training loss for each batch\n",
        "    gt_labels = []  # List to store ground truth labels (predicted age)\n",
        "    pr_labels = []  # List to store predicted labels (model output)\n",
        "\n",
        "    net.train()  # Set the model to training mode (enables dropout and other training-specific behaviors)\n",
        "\n",
        "    for i, data in enumerate(trainloader):\n",
        "        # Get inputs and labels from the data loader\n",
        "        inputs, labels = data[\"image\"], data[\"age\"]\n",
        "        gt_labels.append(labels.cpu().numpy())   # Append ground truth\n",
        "\n",
        "        inputs = inputs.to(device)  # Move data to the appropriate device (CPU or GPU)\n",
        "        labels = labels.to(device)  # Move labels to the appropriate device\n",
        "\n",
        "        # Zero the parameter gradients before each backward pass\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass, calculate loss\n",
        "        outputs = net(inputs)  # Get model predictions\n",
        "\n",
        "        loss = criterion(outputs.squeeze(), labels)  # Calculate L1 loss between predictions and true labels\n",
        "        pr_labels.append(outputs.squeeze().detach().cpu())  # Store predictions (detach to avoid gradients)\n",
        "\n",
        "        # Backward pass and parameter update\n",
        "        loss.backward()  # Backpropagate the loss\n",
        "        optimizer.step()  # Update model weights based on gradients\n",
        "        scheduler.step()  # Update learning rate according to the scheduler\n",
        "\n",
        "        # Print statistics (every 10% of the training data)\n",
        "        running_loss.append(loss.item())\n",
        "        if (i + 1) % (len(trainloader) // 10) == 0:  # Every 10% of the epoch\n",
        "            gt_labels = np.stack(list(itertools.chain.from_iterable(gt_labels))).squeeze()  # Combine ground truth labels\n",
        "            pr_labels = np.stack(list(itertools.chain.from_iterable(pr_labels))).squeeze()  # Combine predictions\n",
        "\n",
        "            # Calculate and print performance metrics (PLCC, SROCC, learning rate)\n",
        "            s = spearmanr(gt_labels, pr_labels)[0]  # Spearman Rank Correlation Coefficient\n",
        "            p = pearsonr(gt_labels, pr_labels)[0]  # Pearson Correlation Coefficient\n",
        "            print('%d, [%d, %d] loss: %.4f\\tPLCC: %.3f\\tSROCC: %.3f\\tlr: %.6f' %\n",
        "                  (epoch + 1, i + 1, len(trainloader), np.mean(running_loss), p, s, optimizer.param_groups[-1]['lr']))\n",
        "\n",
        "            # Clear lists for next iteration within the epoch\n",
        "            gt_labels = []\n",
        "            pr_labels = []\n",
        "            running_loss = []\n",
        "\n",
        "    # Validation loop (after each training epoch)\n",
        "    running_loss = []  # List to store validation loss for each batch\n",
        "    gt_labels = []  # List to store ground truth labels (predicted age)\n",
        "    pr_labels = []  # List to store predicted labels (model output)\n",
        "\n",
        "    net.eval()  # Set the model to evaluation mode (deactivates dropout and other training behaviors)\n",
        "\n",
        "    for i, data in enumerate(valloader):\n",
        "        # Get inputs and labels from the data loader\n",
        "        inputs, labels = data[\"image\"], data[\"age\"]\n",
        "        gt_labels.append(labels.item())  # Append ground truth as single values\n",
        "\n",
        "        inputs = inputs.to(device)  # Move data to the appropriate device\n",
        "        labels = labels.to(device)  # Move labels to the appropriate device\n",
        "\n",
        "        # Forward pass with gradient suppression\n",
        "        with torch.no_grad():\n",
        "            outputs = net(inputs)  # Get model predictions without calculating gradients\n",
        "\n",
        "        pr_labels.append(outputs.squeeze().item())  # Append predictions as single values\n",
        "        loss = criterion(outputs.squeeze(), labels.squeeze())  # Calculate L1 loss\n",
        "        running_loss.append(loss.item())\n",
        "\n",
        "    # Calculate and print validation performance metrics\n",
        "    gt_labels = np.stack(gt_labels)  # Combine ground truth labels\n",
        "    pr_labels = np.stack(pr_labels)  # Combine predictions\n",
        "    s = spearmanr(gt_labels, pr_labels)[0]  # Spearman Rank Correlation Coefficient\n",
        "    p = pearsonr(gt_labels, pr_labels)[0]  # Pearson Correlation Coefficient\n",
        "    print('Validation loss: %.6f\\tPLCC: %.3f\\tSROCC: %.3f' % (np.mean(running_loss), p, s))\n",
        "\n",
        "    # Visualization (optional)\n",
        "    plt.scatter(pr_labels, gt_labels)\n",
        "    plt.xlabel(\"AGE Predicted\")\n",
        "    plt.ylabel(\"AGE GT\")\n",
        "    plt.title(\"PLCC: %.3f\\nSROCC: %.3f\" % (p, s))\n",
        "    plt.show()\n",
        "\n",
        "    # Save the model (optional)\n",
        "    torch.save(net.state_dict(), f\"net_last_e{epoch}.pth\")  # Save model state after each epoch\n",
        "\n",
        "print('Finished Training')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zp4tYtJs9Flk"
      },
      "source": [
        "Evaluate on the test-set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOmgjCHn9IDi"
      },
      "outputs": [],
      "source": [
        "# Testing loop (after training)\n",
        "running_loss = []  # List to store test loss for each batch\n",
        "gt_labels = []  # List to store ground truth labels (predicted age)\n",
        "pr_labels = []  # List to store predicted labels (model output)\n",
        "\n",
        "net.eval()  # Set the model to evaluation mode (deactivates dropout and other training behaviors)\n",
        "\n",
        "for i, data in enumerate(testloader):\n",
        "    # Get inputs and labels from the data loader\n",
        "    inputs, labels = data[\"image\"], data[\"age\"]\n",
        "    gt_labels.append(labels.item())  # Append ground truth as single values\n",
        "\n",
        "    inputs = inputs.to(device)  # Move data to the appropriate device\n",
        "    labels = labels.to(device)  # Move labels to the appropriate device\n",
        "\n",
        "    # Forward pass with gradient suppression\n",
        "    with torch.no_grad():\n",
        "        outputs = net(inputs)  # Get model predictions without calculating gradients\n",
        "\n",
        "    pr_labels.append(outputs.squeeze().item())  # Append predictions as single values\n",
        "    loss = criterion(outputs.squeeze(), labels.squeeze())  # Calculate L1 loss\n",
        "    running_loss.append(loss.item())\n",
        "\n",
        "# Calculate and print test performance metrics\n",
        "gt_labels = np.stack(gt_labels)  # Combine ground truth labels\n",
        "pr_labels = np.stack(pr_labels)  # Combine predictions\n",
        "s = spearmanr(gt_labels, pr_labels)[0]  # Spearman Rank Correlation Coefficient\n",
        "p = pearsonr(gt_labels, pr_labels)[0]  # Pearson Correlation Coefficient\n",
        "print('Test loss: %.6f\\tPLCC: %.3f\\tSROCC: %.3f' % (np.mean(running_loss), p, s))\n",
        "\n",
        "# Visualization (optional)\n",
        "plt.scatter(pr_labels, gt_labels)\n",
        "plt.xlabel(\"AGE Predicted\")\n",
        "plt.ylabel(\"AGE GT\")\n",
        "plt.title(\"PLCC: %.3f\\nSROCC: %.3f\" % (p, s))\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_0hKaVXe7lc"
      },
      "source": [
        "**!ASSIGNMENT!**\n",
        "\n",
        "*Transfer learning*\n",
        "1. Finetune all the layers of the current network (mobilenet_v2).\n",
        "2. Swap out the current model with a new one from [here](https://pytorch.org/vision/main/models.html#classification) that is already trained on Imagenet. Then, fine-tune the network and compare how well it performs on the test-set compared to the current network (mobilenet_v2) using PLCC and SROCC metrics."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
